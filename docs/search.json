[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Systems Programming\n\n\nExploits\n\n\n\n\nAn in-depth explanation of the ‘little-endian’ byte order and what implications that has when exploiting a machine that has this byte order.\n\n\n\n\n\n\nJun 4, 2024\n\n\nNeil Dcruze\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nExploits\n\n\n\n\n[Spoiler Alert]: In this blog series, I will share my insights and solutions for the Capture The Flag (CTF) challenges found on pwnable.kr\n\n\n\n\n\n\nMay 26, 2024\n\n\nNeil Dcruze\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nExploits\n\n\n\n\nThis blog post explores a simplified Stack-Based Buffer Overflow exploit, demonstrating it through a carefully crafted C program and a detailed GDB analysis.\n\n\n\n\n\n\nMay 26, 2024\n\n\nNeil Dcruze\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSystems Programming\n\n\nUnix\n\n\n\n\nWriting a C program that traverses the structures used to represent files (and directories) in UFS2 (Unix File System) and renames a file/directory\n\n\n\n\n\n\nMay 24, 2024\n\n\nNeil Dcruze\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nImage Compression using Singular Value Decomposition\n\n\n\n\n\n\nMar 31, 2023\n\n\nNeil Dcruze\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nTraining a Model to predict Employment for Different Individuals and then Audit the Model to Display Potential Biases across different Racial Groups\n\n\n\n\n\n\nMar 28, 2023\n\n\nNeil Dcruze\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nTwo Different Implementations of the Linear Regression Algorithm\n\n\n\n\n\n\nMar 28, 2023\n\n\nNeil Dcruze\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nAn Implementation and Testing of Kernel Logistic Regression for Binary Classification of Data with Non-Linear Decision Boundaries\n\n\n\n\n\n\nMar 27, 2023\n\n\nNeil Dcruze\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nDifferent Implementations of the Gradient Descent Algorithm, for Linearly Non-Separable Data\n\n\n\n\n\n\nMar 10, 2023\n\n\nNeil Dcruze\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\nMathematics\n\n\n\n\nUsing Mathematics to prove why the Perceptron Learning Rule works\n\n\n\n\n\n\nMar 1, 2023\n\n\nNeil Dcruze\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nAn Implementation of the Perceptron Algorithm\n\n\n\n\n\n\nFeb 28, 2023\n\n\nNeil Dcruze\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/auditing-allocative-bias/index.html",
    "href": "posts/auditing-allocative-bias/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "Loading and Setting Up the Data\nFirst, let us load the data and prepare it for our purposes:\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nSTATE = 'NY'\n\ndata_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download = True)\n\nDownloading data for 2018 1-Year person survey for NY...\n\n\nNow, let us examine the first few rows of the data, to get an idea of it:\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000012\n      2\n      1\n      3802\n      1\n      36\n      1013097\n      145\n      26\n      ...\n      146\n      146\n      21\n      24\n      266\n      263\n      21\n      146\n      265\n      144\n    \n    \n      1\n      P\n      2018GQ0000040\n      2\n      1\n      2702\n      1\n      36\n      1013097\n      43\n      21\n      ...\n      6\n      42\n      43\n      7\n      40\n      6\n      43\n      40\n      42\n      6\n    \n    \n      2\n      P\n      2018GQ0000060\n      2\n      1\n      2001\n      1\n      36\n      1013097\n      88\n      18\n      ...\n      88\n      163\n      161\n      162\n      87\n      12\n      162\n      88\n      87\n      88\n    \n    \n      3\n      P\n      2018GQ0000081\n      2\n      1\n      2401\n      1\n      36\n      1013097\n      109\n      85\n      ...\n      17\n      15\n      111\n      107\n      17\n      196\n      109\n      200\n      198\n      111\n    \n    \n      4\n      P\n      2018GQ0000103\n      2\n      1\n      1400\n      1\n      36\n      1013097\n      83\n      19\n      ...\n      81\n      12\n      80\n      154\n      12\n      80\n      12\n      83\n      152\n      154\n    \n  \n\n5 rows × 286 columns\n\n\n\nEach row in the above dataset corresponds to an individual citizen of New York who filled out the PUMS survey of 2018. Therefore, what we have is a \\(n\\)x\\(p\\) matrix, where \\(n\\) = number of data points, and \\(p\\) = number of features. We can see that there are a bunch of features in this dataset, and all of them might not be pertinent for our analysis. Therefore, we are only going to choose the features which are relevant for this blog post:\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\n\nNow, since we want to predict ESR - Employment Status, based on every relevant feature except the RAC1P - Race, we want to remove these two features from the list of possible_features. This is because one of these features is our target (output), and the we are excluding the other one (race) to study racial bias in our machine learning model:\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nNow that we know:\n\n\nFeatures (features_to_use): the features we wish to use for the prediction\n\nTarget (ESR): the thing we are trying to predict\n\nGroup (RAC1P): the group by which we wish to audit the bias\n\nWe can go ahead and create a BasicProblem:\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nNow we can go ahead and extract our feature matrix, output labels vector, and the group (race) vector as numpy objects:\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nNow, that we have all the necessary data, we can split our data into training data and testing data. Here, 80% of the data will be used for training and the remaining 20% will be used for testing:\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(features, label, group, test_size=0.2, random_state=0)\n\n\n\nBasic Descriptives\nNow, that we have our data set up, we can address some basic descriptives of the data which will aid us in our analysis later on. For this it is useful to make a data frame out of our data:\n\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\nQuestion 1: How many individuals are in the data?\n\nprint(f\"Answer 1: There are {df.shape[0]} people in the training data set!\")\n\nAnswer 1: There are 157573 people in the training data set!\n\n\nQuestion 2: Of these individuals, what proportion have target label equal to 1? In employment prediction, these would correspond to employed individuals.\n\nemployed = df[df['label']==True]\nprint(f\"Answer 2: There are {employed.shape[0]} number of people in the training data set who are employed - have target label equal to True!\")\n\nAnswer 2: There are 73262 number of people in the training data set who are employed - have target label equal to True!\n\n\nQuestion 3: Of these individuals, how many are in each of the groups? Answer 3: We can see a breakdown of how many people of each racial group are employed:\n\nprint(employed.groupby('group').size())\n\ngroup\n1    52646\n2     8030\n3      177\n4        3\n5       66\n6     6778\n7       23\n8     3860\n9     1679\ndtype: int64\n\n\nQuestion 4: In each group, what proportion of individuals have target label equal to 1? Answer 4: The proportion of individuals in each group who are employed (have target label 1):\n\nprint(df.groupby(['group'])[['label']].aggregate([np.mean]).round(2))\n\n      label\n       mean\ngroup      \n1      0.47\n2      0.42\n3      0.43\n4      0.75\n5      0.34\n6      0.50\n7      0.42\n8      0.44\n9      0.37\n\n\nQuestion 5: Check for intersectional trends by studying the proportion of positive target labels broken out by your chosen group labels and an additional group label. For example, if you chose race (RAC1P) as your group, then you could also choose sex (SEX) and compute the proportion of positive labels by both race and sex.  Answer 5: For answering this question, first let us look at the positive labels broken down only by SEX. Then we can look at the intersectionality of RACE and SEX, by looking at positive labels broken down by these two group labels.\n\nprint(df.groupby(['SEX'])[['label']].aggregate([np.mean]).round(2))\n\n    label\n     mean\nSEX      \n1.0  0.49\n2.0  0.44\n\n\nWe can see that: 49% of MEN (1.0) are employed, while 44% of WOMEN (2.0) are employed! Now let us look at the positive target labels broken down by both RACE and SEX:\n\nprint(df.groupby(['group','SEX'])[['label']].aggregate([np.mean]).round(2))\n\n          label\n           mean\ngroup SEX      \n1     1.0  0.50\n      2.0  0.45\n2     1.0  0.40\n      2.0  0.44\n3     1.0  0.46\n      2.0  0.39\n4     1.0  1.00\n      2.0  0.50\n5     1.0  0.39\n      2.0  0.29\n6     1.0  0.54\n      2.0  0.46\n7     1.0  0.35\n      2.0  0.50\n8     1.0  0.48\n      2.0  0.41\n9     1.0  0.36\n      2.0  0.38\n\n\nWe can visualize this data using a bar chart for easier understanding:\n\nbplot = sns.barplot(data=df, x=\"group\", y=\"label\", hue=\"SEX\", errorbar=('ci', 0))\nbplot.set(xlabel=\"Different Racial Groups\", ylabel=\"Proportion of People Employed\")\nplt.show(bplot)\n\n\n\n\nTherefore, it is apparent that the proportion of women who are employed, is less than the proportion of men who are employed, in almost all racial groups except racial group 2, 7, and 9. Not only that, some racial groups have a really high disparity between the proportion of men who are employed versus the proportion of women who are employed. For instance, in racial group 4: the proportion of men who are employed is roughly 100%, while the proportion of women who are employed is only 50%.\n\n\nTraining the Model\nNow that we have answered the basic descriptive questions, we are ready to train our model on the training data. For this blog post, the chosen machine learning model is: Logistic Regression, and we will be tuning the polynomial features (number of degrees) in our Logistic Regression model. Therefore, our workflow is going to be:\n\n\nCreating a function that will utilize Pipeline to make it easier for us to construct a Logistic Regression model with certain number of polynomial features (a certain number of degrees)\n\nUsing cross-validation to select the best degree (number of polynomial features)\n\nCreating a Logistic Regression model with the best number of degrees that we found in Step (2) and fitting it on our training data\n\nChecking the performance of our model on the testing data\n\nLet us get started with creating the function which utilizes Pipeline, for easier construction of our Logistic Regression models:\n\ndef polyLR(deg):\n  return Pipeline([(\"poly\", PolynomialFeatures(degree = deg)),\n                   (\"LR\", LogisticRegression(penalty = None, max_iter = 1000))])\n\nNow, we can use cross-validation to select which degree (number of polynomial features) works best for our data. The main principle behind cross-validation is that you can divide your training data into \\(k\\) folds (chunks), and then train your model over \\(k-1\\) chunks and then validate it over the last remaining chunk. Therefore, if you divide your training data into \\(4\\) chunks - let us say - then each one of the \\(4\\) chunks will act as the validation data once, with the rest of the chunks being used as training data. Therefore, in case of \\(4\\) chunks, the fit function will be called \\(4\\) times. Now, we can find the mean cross-validation score for varying degrees (number of polynomial features), and the degree which has the highest cross-validation score is most likely to perform the best on the testing data:\n\nfor deg in range(5):\n    plr = polyLR(deg = deg)\n    cv_scores = cross_val_score(plr, X_train, y_train, cv = 5)\n    mean_score = cv_scores.mean()\n    print(f\"Polynomial Degree = {deg}, Score = {mean_score.round(3)}\")\n\nPolynomial Degree = 0, Score = 0.535\nPolynomial Degree = 1, Score = 0.772\nPolynomial Degree = 2, Score = 0.811\nPolynomial Degree = 3, Score = 0.809\nPolynomial Degree = 4, Score = 0.806\n\n\nTherefore, for our purposes it seems that Polynomial Degree = \\(2\\) yields the best results during our cross-validation, so we can assume that this degree is also likely to perform the best on our testing data. Now, we can go ahead and create a Logistic Regression model with degrees = \\(2\\):\n\nplr = polyLR(deg = 2)\nplr.fit(X_train, y_train)\nprint(f\"Score on Training Data: {plr.score(X_train, y_train).round(3)}\")\n\nScore on Training Data: 0.811\n\n\nNow, finally we can check the performance of our model on the testing data:\n\nprint(f\"Score on Testing Data: {plr.score(X_test, y_test).round(3)}\")\n\nScore on Testing Data: 0.811\n\n\nTherefore, we can see that we achieved approximately \\(81\\)% accuracy on our testing data!\n\n\nAuditing the Model\nNow that our model is trained, we can perform an audit on the test data where we can answer questions on our testing data. It is helpful to use a confusion matrix while answering these questions. A confusion matrix, is a matrix containing information about the model’s prediction and the kind of errors it makes:\n\n\nTrue Negative: Negative Data + Classified Negative by Model\n\nFalse Positive: Negative Data + Wrongly Classified Positive by Model\n\nFalse Negative: Positive Data + Wrongly Classified Negative by Model\n\nTrue Positive: Positive Data + Classified Positive by Model\n\n\ny_pred = plr.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(f\"The Confusion Matrix:\\n {cm}\")\ntn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\nprint(f\"\\nTrue Negative: {tn}\")\nprint(f\"False Positive: {fp}\")\nprint(f\"False Negative: {fn}\")\nprint(f\"True Positive: {tp}\")\n\nThe Confusion Matrix:\n [[16235  4952]\n [ 2491 15716]]\n\nTrue Negative: 16235\nFalse Positive: 4952\nFalse Negative: 2491\nTrue Positive: 15716\n\n\nThat above is our confusion matrix with the information we talked about above. Now, it is useful to define some concepts and associated formulae:\n\n\nOverall Accuracy: proportion of correct predictions made over the total number of predictions made \n\n\\(\\text{Overall Accuracy} = \\frac{TN + TP}{TN + TP + FP + FN}\\)\n\n\n\nPositive Predictive Value (PPV): probability that a positive prediction made by the model is actually correct \n\n\\(\\text{PPV} = \\frac{TP}{TP + FP}\\)\n\n\n\nFalse Positive Rate (FPR): proportion of negative examples that were incorrectly classified as positve\n\n\\(\\text{FPR} = \\frac{FP}{FP + TN}\\)\n\n\n\nFalse Negative Rate (FNR): proportion of positive examples that were incorrectly classified as negative \n\n\\(\\text{FNR} = \\frac{FN}{FN + TP}\\)\n\n\n\nOverall Measures\n\nWhat is the overall accuracy of our model?\n\n\noverallAccuracy = ((tn + tp)/(tn + tp + fp + fn)).round(3)\nprint(f\"Overall Accuracy of the Model is {np.round(overallAccuracy*100,3)}%\")\n\nOverall Accuracy of the Model is 81.1%\n\n\n\nWhat is the Positive Predictive Value (PPV) of our model?\n\n\nppv = ((tp)/(tp + fp)).round(3)\nprint(f\"PPV of the Model: {ppv}\")\nprint(f\"This means that {ppv} of all positive predictions made by the model are actually correct!\")\n\nPPV of the Model: 0.76\nThis means that 0.76 of all positive predictions made by the model are actually correct!\n\n\n\nWhat are the overall False Positive and False Negative Rates (FPR and FNR) for our model?\n\n\nfpr = ((fp)/(fp + tn)).round(3)\nfnr = ((fn)/(fn + tp)).round(3)\nprint(f\"FPR of the Model: {fpr}\")\nprint(f\"This means that {fpr} of all negative examples were wrongly classified as positive!\")\nprint(f\"\\nFNR of the Model: {fnr}\")\nprint(f\"This means that {fnr} of all positive examples were wrongly classified as negative!\")\n\nFPR of the Model: 0.234\nThis means that 0.234 of all negative examples were wrongly classified as positive!\n\nFNR of the Model: 0.137\nThis means that 0.137 of all positive examples were wrongly classified as negative!\n\n\n\n\nBy-Group Measures\nNow, we can go ahead and look at the: Overall Accuracy, PPV, FPR, and FNR for each the 9 subgroups (the different racial groups). We cannot calculate the statistics for Group 4: Alaskan Natives due to lack of sufficient sample:\n\nfor i in list(range(1,4)) + list(range(5, 10)):\n    print(f\"\\nGroup {i}:\")\n    ix = X_test[group_test == i, :]\n    y_testx = y_test[group_test == i]\n    y_predx = plr.predict(ix)\n    cm = confusion_matrix(y_testx, y_predx)\n    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n    overallAccuracy = (((tn + tp)/(tn + tp + fp + fn)) * 100).round(2)\n    ppv = ((tp)/(tp + fp)).round(3)\n    fpr = ((fp)/(fp + tn)).round(3)\n    fnr = ((fn)/(fn + tp)).round(3)\n    print(f\"Overall Accuracy: {overallAccuracy}%\")\n    print(f\"PPV of the Model: {ppv}\")\n    print(f\"This means that {ppv} of all positive predictions made by the model are actually correct!\")\n    print(f\"FPR of the Model: {fpr}\")\n    print(f\"This means that {fpr} of all negative examples were wrongly classified as positive!\")\n    print(f\"FNR of the Model: {fnr}\")\n    print(f\"This means that {fnr} of all positive examples were wrongly classified as negative!\")\n\n\nGroup 1:\nOverall Accuracy: 81.62%\nPPV of the Model: 0.772\nThis means that 0.772 of all positive predictions made by the model are actually correct!\nFPR of the Model: 0.223\nThis means that 0.223 of all negative examples were wrongly classified as positive!\nFNR of the Model: 0.139\nThis means that 0.139 of all positive examples were wrongly classified as negative!\n\nGroup 2:\nOverall Accuracy: 80.6%\nPPV of the Model: 0.734\nThis means that 0.734 of all positive predictions made by the model are actually correct!\nFPR of the Model: 0.24\nThis means that 0.24 of all negative examples were wrongly classified as positive!\nFNR of the Model: 0.133\nThis means that 0.133 of all positive examples were wrongly classified as negative!\n\nGroup 3:\nOverall Accuracy: 73.4%\nPPV of the Model: 0.667\nThis means that 0.667 of all positive predictions made by the model are actually correct!\nFPR of the Model: 0.278\nThis means that 0.278 of all negative examples were wrongly classified as positive!\nFNR of the Model: 0.25\nThis means that 0.25 of all positive examples were wrongly classified as negative!\n\nGroup 5:\nOverall Accuracy: 78.72%\nPPV of the Model: 0.682\nThis means that 0.682 of all positive predictions made by the model are actually correct!\nFPR of the Model: 0.241\nThis means that 0.241 of all negative examples were wrongly classified as positive!\nFNR of the Model: 0.167\nThis means that 0.167 of all positive examples were wrongly classified as negative!\n\nGroup 6:\nOverall Accuracy: 78.42%\nPPV of the Model: 0.733\nThis means that 0.733 of all positive predictions made by the model are actually correct!\nFPR of the Model: 0.324\nThis means that 0.324 of all negative examples were wrongly classified as positive!\nFNR of the Model: 0.107\nThis means that 0.107 of all positive examples were wrongly classified as negative!\n\nGroup 7:\nOverall Accuracy: 70.59%\nPPV of the Model: 0.5\nThis means that 0.5 of all positive predictions made by the model are actually correct!\nFPR of the Model: 0.417\nThis means that 0.417 of all negative examples were wrongly classified as positive!\nFNR of the Model: 0.0\nThis means that 0.0 of all positive examples were wrongly classified as negative!\n\nGroup 8:\nOverall Accuracy: 77.95%\nPPV of the Model: 0.722\nThis means that 0.722 of all positive predictions made by the model are actually correct!\nFPR of the Model: 0.267\nThis means that 0.267 of all negative examples were wrongly classified as positive!\nFNR of the Model: 0.164\nThis means that 0.164 of all positive examples were wrongly classified as negative!\n\nGroup 9:\nOverall Accuracy: 85.9%\nPPV of the Model: 0.77\nThis means that 0.77 of all positive predictions made by the model are actually correct!\nFPR of the Model: 0.144\nThis means that 0.144 of all negative examples were wrongly classified as positive!\nFNR of the Model: 0.136\nThis means that 0.136 of all positive examples were wrongly classified as negative!\n\n\n\n\nBias Measures\n\nIs the model approximately calibrated?\nThe results clearly show that the PPV varies across the different racial groups. Therefore, our model is not calibrated. For example, if we compared Group 1 (White only): PPV = \\(0.772\\) and Group 2 (Black only): PPV = \\(0.734\\). Therefore, \\(\\text{PPV}_\\text{White} > \\text{PPV}_\\text{Black}\\), which means that the probability that a positive prediction made by the model is correct, is higher for white people, compared to black people.\n\n\nDoes the model satisfy approximate error rate balance?\nThe model does not satisfy approximate error rate balance. This is because the FPR and FNR is different across different racial groups. For example, analyzing Group 1 (White only) and Group 2 (Black only): \\((\\text{FPR}_\\text{White} = 0.223) < (\\text{FPR}_\\text{Black}= 0.24)\\), and \\((\\text{FNR}_\\text{White} = 0.139) > (\\text{FNR}_\\text{Black}= 0.133)\\). Therefore, for FPR: the model is more likely to wrongly classify negative examples (not employed) as positive (employed) for Black people compared to White people. For FNR: the model is more likely to wrongly classify positive examples (employed) as negative (not employed) for White people compared to Black people. Therefore, the model makes different types of mistakes more often, across different racial groups.\n\n\nDoes the model satisfy statistical parity?\nSince, our model has different values of PPV for different racial groups - our model does not satisfy statistical parity!\n\n\n\n\nConcluding Discussions\n\nWhat groups of people could stand to benefit from a system? hat kinds of companies might want to buy your model for commercial use?\nSince this model predicts whether a person is employed or not, based on other demographic factors. This model could be used to automate or cross-check the process of providing unemployment benefits to individuals. Since this model is more likely to classify unemployed Black people as employed, and more likely to classify employed White people as unemployed, if there were a model which gave people unemployment benefits based on the results of this model - White people would benefit!\n\n\nWhat could be the impact of deploying your model for large-scale prediction in commercial or governmental settings? Do you feel that your model displays problematic bias?\nDeploying a model like this for large-scale prediction in commercial or governmental settings will be problematic. This is because, as discussed above: the model will make different kinds of mistakes for different racial groups. Therefore, certain racial groups will benefit more than others - for instance, if the model is used to decide who gets unemployment benefits. Furthermore, this model displays problematic bias. As discussed in the Bias Measures the model is not (does not satisfy):\n\n\nCalibrated: probability that a positive prediction made by model is correct is different across racial groups\n\nApproximate Error Rate Balance: will make different kinds of mistakes for different racial groups\n\nStatistical Parity: model does not satisfy Statistical Parity (different PPV) for different racial groups\n\n\n\nAre there other potential problems associated with deploying your model that make you uncomfortable? How would you propose addressing some of these problems?\nIf this model were to be used for government purposes: deciding unemployment benefits (mentioned above), I would not feel very comfortable. This is because the model first of all has its own problems and biases, and furthermore automating a process which affects an individual (and humans) so much is very inhumane, as the errors of the model can deeply affect people. If the model did not exhibit the problematic biases discussed above, then maybe the model would be slightly better for deployment. However, it would still not be ideal according to me."
  },
  {
    "objectID": "posts/gradient-descent/index.html",
    "href": "posts/gradient-descent/index.html",
    "title": "Gradient Descent for Optimizing Logistic Regression",
    "section": "",
    "text": "Introduction to Gradient Descent Algorithm\nA convex function - in simple terms - can be explained as a function which is “bowl shaped”. The interesting thing about convex functions is that if you keep taking steps towards what feels as “lower ground” - lower value of that convex function - you are ultimately bound to reach the minimum. A property of convex functions is that if there exists a local minima - \\(w^*\\) - it is also the global minimum - \\(w\\). Due to this property, we can “greedily” use the Gradient Descent algorithm, and be sure that we are ultimately bound to reach the global mininimum.\nIn this blog post, we are going to explore three different approaches of Gradient Descent:\n\n\nRegular Gradient Descent\n\nStochastic Gradient Descent\n\nStochastic Gradient Descent with Momentum\n\nWe will see how these three different approaches of Gradient Descent perform - how long they take to converge and find the minimum! Furthermore, we will also perform some experiments which will highlight some of the nuances of these approaches.\nHere is an image of how a convex function looks, for visualization purposes:\n\n\n\n\n\nImplementing Gradient Descent\n\n#implementing the necessary packages\nfrom LogisticRegression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n# logistic regression tends to involve a lot of log(0) and things that wash out in the end. \nnp.seterr(all='ignore') \n\n{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n\n\nIn the following section, we are using the make_blobs function of sklearn.datasets to create synthetic data for our experimentation. In this case, we are creating two clusters of data, which are centered around two different areas - and have different labels \\(0\\) and \\(1\\). These two clusters purposefully have overlap, and are not linearly separable. This means that one cannot imagine a straight line which perfectly divides the data points into two regions - one group or the other.\n\n#arbitrarily setting the seed number to 12345 to ensure reproducibility\nnp.random.seed(12345)\n\n#setting the number of points we want\nn = 200\n\n#setting the number of features\nfeatures = 2\n\n#using make_blobs function to create 2 clusters of 200 data points - centered   \n#randomly around (-1,-1) and (1,1) - making them linearly inseparable \nX, y = make_blobs(n_samples = n, n_features = features, centers = [(-1,-1),(1,1)])\n\n\n#plotting the clusters generated by the make_blobs function\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n#creating an instance of the LogisticRegression class\nLR = LogisticRegression()\n\nWe are calling the fit method of the LogisticRegression class. This method uses the logistic loss function to calculate the loss, and updates the weight vector, using the Gradient Descent algorithm. The gradient function for the logistic loss function, has been borrowed from the  lecture notes  on Gradient Descent!\n\nLR.fit(X, y, 0.1, 1000)\n\nFor reference, the code had saved the first weight vector. Therefore, we can visualize how well the first linear classifier performed, and its associated loss.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = LR.draw_line(LR.initialW, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\ntitle = plt.gca().set_title(f\"Loss = {LR.loss_history[0]}\")\n\n\n\n\nWe can then compare this to the visualization of the final weight vector (after performing gradient descent) - how it performed the linear classification, and its associated loss.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = LR.draw_line(LR.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\ntitle = plt.gca().set_title(f\"Loss = {LR.loss_history[-1]}\")\n\n\n\n\nTherefore, we can clearly see how the final set of weight vector (inclusive of bias) has a much less associated loss, and does a much better job at performing the linear classification. This can also be seen visually, by looking at the classification in the image above (with the final weight vector), and the one above it (with the initial - random - weight vector).\n\n\nExplanation of Important Elements of the Regular Gradient Descent Algorithm\nIn this section of the blog post, we are going to look at some of the functions which are used in performing our Logistic Regression using Regular Gradient Descent!\n\nfit method:\n\ndef fit(self, X, y, alpha, max_epochs):\n        #creating modified feature matrix X_ with column of 1s\n        X_ = np.append(X, np.ones((X.shape[0],1)), axis=1)\n        #initializing random weight vectors\n        self.w = np.random.rand(X_.shape[1])\n        self.initialW = self.w.copy() #solely for visualization purposes\n        #adding loss+score associated with initialized weight vector to respective vectors\n        self.loss_history.append(self.loss(X_,y))\n        self.score_history.append(self.score(X_,y)\n\n        for i in range(max_epochs):\n            #updating the weight vector - the gradient descent update\n            gradient = self.gradient_logistic_loss(X_, y)\n            self.w -= alpha*gradient\n\n            #appending the loss and score for the updated weight vector\n            self.loss_history.append(self.loss(X_,y))\n            self.score_history.append(self.score(X_,y))\n\n            #checking the conditions to terminate the loop - when overall improvement is minimum\n            #comparing the latest added loss_history with the second_latest added loss_history\n            if np.isclose(self.loss_history[-1], self.loss_history[-2]):\n                break\nThe above mentioned method is the main one, which is called to perform gradient descent, and find the optimal parameters of the weight vector \\(w\\) - which minimizes our loss - using logistic loss! In the above function, first we pad the feature matrix \\(X\\) with a column of \\(1\\)s. This is done to ease the mathematics we perform, as this allows us to include the bias in the weight vector - \\(w\\) - and perform matrix operations. We run the gradient descent algorithm for max_epochs, which is the maximum number of steps that the user provides us. Another situation in which we break the loop is provided in the line:\nif np.isclose(self.loss_history[-1], self.loss_history[-2]):\n    break\nThat is, when the overall improvement in the previous and current losses is extremely low - it is a good time to stop performing gradient descent.\n\nloss method:\n\ndef loss(self, X, y):\n    y_hat = X@self.w\n    return self.logistic_loss(y_hat, y).mean()\nThe above mentioned method is the loss method aka the Empirical Risk function! We first get a predicted vector y_hat for the \\(n\\) data points. An important thing to note is that for the prediction vector, we use the actual values of the \\(\\text{X}@\\text{self.w}\\), rather than converting them to \\(0\\)s and \\(1\\)s - as we do in our predict function. This is because the logistic loss function’s genius lies on the actual values of our prediction, and converting to \\(0\\)s and \\(1\\)s would rid us of the range of ways in which our prediction affects the loss!\n\ngradient_logistic_loss method:\n\ndef gradient_logistic_loss(self, X, y):\n    #using the actual predictions - rather than 0s and 1s \n    #to utilize the potential of the logistic loss function\n    y_hat = X@self.w\n    tempYDifference = self.sigmoid(y_hat) - y\n    output = np.zeros((X.shape[1],))\n    for i in range(X.shape[0]):\n        output+=np.dot(tempYDifference[i], X[i])\n    output = output/X.shape[0]\n    return output\nThis is arguably the most important function in the entire implementation of the Gradient Descent algorithm, as it forms the backbone for the entire update. Using multivariable calculus, it was found that the gradient of the logistic loss function is:\n\n\n\n\nSource - Philip Chodrow (Lecture Notes on Gradient Descent 02/22)\n\n\n\nTherefore, we first get y_hat prediction vector - again the actual values, rather than \\(0\\)s and \\(1\\)s - to use the range of values in our predictions. Now, the mathematical idea is that for each value of \\(\\sigma(\\hat y_i) - y_i\\), we want to multiply it by the corresponding row of the feature matrix \\(X\\), that is \\(X_i\\). So, at each stage, we are multiplying a \\(1\\)x\\(1\\) value, with a \\(1\\)x\\(p\\) row, therefore resulting in a \\(1\\)x\\(p\\) row. Then we want to add all these \\(n\\), \\(1\\)x\\(p\\) rows, that is adding the respective elements, and then taking the mean, which at this point is simply division by the number of rows. Therefore, we run a loop which takes the \\(i\\)th value of \\(\\sigma(\\hat y_i) - y_i\\) and does a dot product of that with the \\(i\\)th row of the feature matrix \\(X\\). We sum the value of these multiplications for all values of \\(i\\) - number of rows in the feature matrix \\(X\\) and then divide the output by the \\(X.shape[0]\\) - the number of rows in the feature matrix - to get the average. Overall, this means that we are multiplying a \\(1\\)x\\(n\\) matrix, with a \\(n\\)x\\(p\\) matrix - resulting in a \\(1\\)x\\(p\\) matrix - exactly what we want.\n\n\nStochastic Gradient Descent\ndef fit_stochastic(self, X, y, alpha, max_epochs, batch_size, momentum = False):\n    #creating modified feature matrix X_ with column of 1s \n    X_ = np.append(X, np.ones((X.shape[0],1)), axis=1)\n    #initializing random weight vectors\n    self.w = np.random.rand(X_.shape[1])\n    self.initialW = self.w.copy()\n    #adding loss+score associated with initialized weight vector to respective vectors\n    self.loss_history.append(self.loss(X_,y))\n    self.score_history.append(self.score(X_,y)\n\n    #setting the beta value of momentum - based on momentum True or False\n    beta = 0\n    if(momentum):\n        beta = 0.8\n\n    #creating a temporary variable which stores the previous weight vector - useful for implementing momentum\n    prevWeightVec = 0\n\n    #setting n as the number of rows - data points - in our feature matrix X_\n    n = X_.shape[0]\n\n    #maximum number of time we are cycling through the data - max_epochs \n    #np.arange is similar to range used in above fit function\n    for j in np.arange(max_epochs):\n        #creating a list of all possible data points and shuffling\n        order = np.arange(n)\n        np.random.shuffle(order)\n\n        #this loop goes over the order - total number of rows (data points)\n        #and then creates batches out of those orders, where n (total data points) // batch_size + 1\n        for batch in np.array_split(order, n // batch_size + 1):\n            x_batch = X_[batch,:]\n            y_batch = y[batch]\n            #performing the stochastic gradient\n            gradient = self.gradient_logistic_loss(x_batch, y_batch)\n            tempWeightVec = self.w\n            self.w -= (alpha * gradient) + (beta * (self.w - prevWeightVec))\n            prevWeightVec = tempWeightVec\n\n        #appending the loss and score for the updated weight vector\n        self.loss_history.append(self.loss(X_,y))\n        self.score_history.append(self.score(X_,y))\n\n        #checking the conditions to terminate the loop - when overall improvement is minimum\n        if np.isclose(self.loss_history[-1], self.loss_history[-2]):\n            break\nIn addition to regular gradient descent, this blog post also implements and looks at the performance of a Stochastic Gradient Descent. Similar, to the regular gradient descent, we still run over the data a fixed number of times - max_epochs. However, in each iteration, instead of passing the complete feature matrix to the gradient function, we randomly pass a subset of certain size - batch size - to the gradient function. We then update the weight using the gradient of this batch size subset passed to the gradient function. We keep on doing this until we have made the updates for all set of points. Therefore, the weight vector \\(w\\) gets updates multiple times in each epoch based on smaller subsets of the feature matrix and the related true labels. Furthermore, there is also a parameter called momentum in this implementation of the Stochastic Gradient Function, which controls the \\(\\beta\\) parameter. This is done\n\n\nEvaluating the Performance of All Three Algorithms\nHypothesis:\nStochastic Gradient Descent (with and without Momentum) will converge faster than Regular Gradient Descent. However, once it finds the optimum value - Regular Gradient Descent will “settle down”, while Stochastic Gradient Descent will “bounce around” a little.\n\n#Regular Gradient Descent\nLR_G = LogisticRegression()\nLR_G.fit(X, y, alpha = 0.5, max_epochs = 1000)\nnum_steps = len(LR_G.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_G.loss_history, label = \"Regular Gradient Descent\")\n\n#Stochastic Gradient Descent\nLR_S = LogisticRegression()\nLR_S.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, batch_size = 10, momentum = False) \nnum_steps = len(LR_S.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_S.loss_history, label = \"Stochastic Gradient Descent\")\n\n#Stochastic Gradient Descent (with Momentum)\nLR_SM = LogisticRegression()\nLR_SM.fit_stochastic(X, y, alpha = 0.5, max_epochs = 1000, batch_size = 10, momentum = True) \nnum_steps = len(LR_SM.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_SM.loss_history, label = \"Stochastic Gradient Descent (with Momentum)\")\n\nplt.loglog()\nlegend = plt.legend() \n\n\n\n\nOur hypothesis is true, and we can see that the Stochastic Gradient Descent (with and without Momentum) converge faster to the minimizing solution, however they “bounce around” a little bit near the optimum solution. However, Regular Gradient Descent converges slower, but once it finds the optimum it “settles down”.\n\n\nEffects of \\(\\alpha\\) - Learning Rate - in Finding a Minimum Solution\nIn this section, we will evaluate the Regular Gradient Descent function - fit - in a 2 feature matrix, and look at how the learning rate - \\(\\alpha\\) - affects the convergence to a minimizer. More specifically, we want to see if a “very large” value of \\(\\alpha\\), can prohibit us from ever converging to a minimum!\n\n#implementing the necessary packages\nfrom LogisticRegression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n# logistic regression tends to involve a lot of log(0) and things that wash out in the end. \nnp.seterr(all='ignore') \n\n{'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}\n\n\nWe can use the make_blobs function to create \\(200\\) data points, with \\(2\\) features each. Because of this, we can say that \\(X\\) is a \\(200\\)x\\(2\\) matrix.\n\n#arbitrarily setting the seed number to 12345 to ensure reproducibility\nnp.random.seed(12345)\n\n#setting the number of points we want\nn = 200\n\n#setting the number of features\nfeatures = 2\n\n#using make_blobs function to create 2 clusters of 200 data points - centered   \n#randomly around (-1,-1) and (1,1) - making them linearly inseparable \nX, y = make_blobs(n_samples = n, n_features = features, centers = [(-1,-1),(1,1)])\n\n\n#setting the first value of alpha\nLR_first = LogisticRegression()\nLR_first.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n#setting the size of the figure\nplt.rcParams[\"figure.figsize\"] = (15,5)\n\n#creating two subplots\nfig, ax = plt.subplots(1, 2)\n\n#plotting the first subplot\nfig = ax[0].scatter(X[:,0], X[:,1], c = y)\nxlab = ax[0].set_xlabel(\"Feature 1\") \nylab = ax[0].set_ylabel(\"Feature 2\") \nLR_first.draw_line(LR_first.w, -2, 2, subplot = ax[0])\ntitle = ax[0].set_title(f\"Loss = {LR_first.loss_history[-1]}\") \n\n#plotting the second subplot\nnum_steps = len(LR_first.loss_history)\nfig = ax[1].plot(np.arange(num_steps) + 1, LR_first.loss_history)\nplt.title(\"Alpha = 0.1\")\n\nplt.show()\n\n\n\n\n\n#setting the first value of alpha\nLR_second = LogisticRegression()\nLR_second.fit(X, y, alpha = 5, max_epochs = 1000)\n\n#setting the size of the figure\nplt.rcParams[\"figure.figsize\"] = (15,5)\n\n#creating two subplots\nfig, ax = plt.subplots(1, 2)\n\n#plotting the first subplot\nfig = ax[0].scatter(X[:,0], X[:,1], c = y)\nxlab = ax[0].set_xlabel(\"Feature 1\") \nylab = ax[0].set_ylabel(\"Feature 2\") \nLR_second.draw_line(LR_second.w, -2, 2, subplot = ax[0])\ntitle = ax[0].set_title(f\"Loss = {LR_second.loss_history[-1]}\") \n\n#plotting the second subplot\nnum_steps = len(LR_second.loss_history)\nfig = ax[1].plot(np.arange(num_steps) + 1, LR_second.loss_history)\nplt.title(\"Alpha = 5\")\n\nplt.show()\n\n\n\n\n\n#setting the first value of alpha\nLR_third = LogisticRegression()\nLR_third.fit(X, y, alpha = 45, max_epochs = 1000)\n\n#setting the size of the figure\nplt.rcParams[\"figure.figsize\"] = (15,5)\n\n#creating two subplots\nfig, ax = plt.subplots(1, 2)\n\n#plotting the first subplot\nfig = ax[0].scatter(X[:,0], X[:,1], c = y)\nxlab = ax[0].set_xlabel(\"Feature 1\") \nylab = ax[0].set_ylabel(\"Feature 2\") \nLR_third.draw_line(LR_third.w, -2, 2, subplot = ax[0])\ntitle = ax[0].set_title(f\"Loss = {LR_third.loss_history[-1]}\") \n\n#plotting the second subplot\nnum_steps = len(LR_third.loss_history)\nfig = ax[1].plot(np.arange(num_steps) + 1, LR_third.loss_history)\nplt.title(\"Alpha = 45\")\n\nplt.show()\n\n\n\n\nTherefore, we can clearly see that when we choose a very small \\(\\alpha\\) size = \\(0.1\\), the Gradient Descent converges, it just takes more time to converge. Then, if we choose a relatively small \\(\\alpha\\) size = \\(5\\), then the Gradient Descent converges a little faster. However, if choose a really large \\(\\alpha\\) size = \\(45\\), then Gradient Descent just keeps on oscilatting, and it never converges!\n\n\nChoice of Batch Size in Stochastic Gradient Descent’s Convergence\nIn this section, we will be looking at how the batch size in Gradient Descent affects how quickly the algorithm converges! For this experiment, we will be working with a feature matrix with \\(10\\) features. Now, let us set up the data for experimentation!\n\n#implementing the necessary packages\nfrom LogisticRegression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n# logistic regression tends to involve a lot of log(0) and things that wash out in the end. \nnp.seterr(all='ignore') \n\n#arbitrarily setting the seed number to 12345 to ensure reproducibility\nnp.random.seed(12345)\n\n#setting the number of points we want\nn = 200\n\n#setting the number of features\nfeatures = 10\n\n#using make_blobs function to create 2 clusters of 200 data points - centered   \n#randomly in 2 centers - since we are dealing with binary classification \nX, y = make_blobs(n_samples = n, n_features = features, centers = 2)\n\nNow, let us plot some graphs, which will allow us to understand the effect of batch size on convergence speed. We will be dealing with three cases, one where the batch size is really small, meaning that there will be a lot of batches in each epoch. Next, we would be dealing with a moderately sized batch size, meaning that there will be a medium number of batches in each epoch. And finally, for our extreme case, we will be looking at a very large batch size (equal to the number of data points), this would mean that there will only be \\(1\\) batch, for each epoch, technically making the Stochastic Gradient Descent a Regular Gradient Descent, since all the points would be used up in the same batch, rather than dividing and updating.\n\nLR_smallSize = LogisticRegression()\nLR_smallSize.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 5)\nnum_steps = len(LR_smallSize.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_smallSize.loss_history, label = \"Batch Size = 5\")\n\nLR_mediumSize = LogisticRegression()\nLR_mediumSize.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 15)\nnum_steps = len(LR_mediumSize.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_mediumSize.loss_history, label = \"Batch Size = 15\")\n\nLR_mediumSizeTwo = LogisticRegression()\nLR_mediumSizeTwo.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 50)\nnum_steps = len(LR_mediumSizeTwo.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_mediumSizeTwo.loss_history, label = \"Batch Size = 50\")\n\n#technically Regular Gradient Descent - since there would only be one batch \nLR_largeSize = LogisticRegression()\nLR_largeSize.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 200)\nnum_steps = len(LR_largeSize.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_largeSize.loss_history, label = \"Batch Size = 200\")\n\nplt.loglog()\nlegend = plt.legend()\n\n\n\n\nTherefore, we can see that choosing a small batch size usually leads to faster convergence. In our last case, where batch_size = \\(200\\), that basically means that everything is in one batch - Regular Gradient Descent. However, upon further research I found out that choosing a very small batch size is not necessarily good, and a very small batch size might lead to introduction of randomness or “noise” in the update process. This can lead to slow convergence (contrary to what I mentioned above), and can increase computational cost.\n\n\nIntroduction of Momentum and its Effects on Convergence Speed\nIn this section, we will be looking at the effects of “momentum” on the convergence speed of Stochastic Gradient descent. Now, let us look at the convergence speed of two implementations of the Stochastic Gradient Descent, one without the momentum parameter, and one with the momentum parameter!\n\nLR_noM = LogisticRegression()\nLR_noM.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 15, momentum = False)\nnum_steps = len(LR_noM.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_noM.loss_history, label = \"Stochastic Gradient (without Momentum)\")\n\nLR_M = LogisticRegression()\nLR_M.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 15, momentum = True)\nnum_steps = len(LR_M.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_M.loss_history, label = \"Stochastic Gradient (with Momentum)\")\n\nplt.loglog()\nlegend = plt.legend()\n\n\n\n\nTherefore, we can see that the Stochastic Gradient (with Momentum) converged faster than Stochastic Gradient (without Momentum). However, upon further research I found out that it is important to set the momentum parameter right. Choosing a very high value for the momentum parameter might lead to overshooting the minimum and oscillate around it. At the same time, choosing a very small value for the momentum parameter might lead to the algorithm taking a lot of time to converge!"
  },
  {
    "objectID": "posts/kernel-logistic-regression/index.html",
    "href": "posts/kernel-logistic-regression/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Introduction\nIn the previous blog post, we looked at Logistic Regression with Gradient Descent, which worked well for binary classification of data with linear decision boundaries. However, in this blog post we aim to:\n\n\nAppreciate the limitations of that approach by working with data that does not have Linear Decision Boundaries\n\nImplement Kernel Logistic Regression and use that for binary classification of data that does not have Linear Decision Boundaries\n\nBefore we get started, let us get an understanding of what data with “non-linear decision boundaries” means. So far, we have been dealing with the “construction” of the best line which separates the data into one group or the other, however if you look at this data: \n\n\n\nVisually, it is pretty apparent that the data follows a circular pattern, with the two “classes” having different radii (varying distance from the center). So it is pretty apparent that if we tried our implementation of Logistic Regression with Gradient Descent, it would not do well since by the inherent nature of this data - there are no linear decision boundaries.\n\n\nLogistic Regression versus Kernel Logistic Regression on Data with Non-Linear Decision Boundaries\nIn this section, we aim to understand the performance difference of Logistic Regression and Kernel Logistic Regression by generating some data which has clear non-linear decision boundaries. For the Kernel Logistic Regression we will be using the code that I wrote, however for the Logistic Regression we will be using scikit-learn’s model. Let us go ahead and generate some synthetic data:\n\nfrom KernelLogisticRegression import KernelLogisticRegression\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport pandas as pd\nnp.seterr(all=\"ignore\")\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\n\n#visualizing the data\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nClearly, the data above has non-linear decision boundaries and follows a “moon” like shape. Now, let us go ahead and fit our version of Kernel Logistic Regression and scikit-learn’s Logistic Regression on the data:\n\n#Regular logistic Regression \nLR = LogisticRegression()\nLR.fit(X, y)\n\n#Kernel Logistic Regression\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10)\nKLR.fit(X, y)\n\nNow, we can go ahead and visualize how our two models performed:\n\nplt.rcParams[\"figure.figsize\"] = (15, 5)\nfig, ax = plt.subplots(1, 2)\n\n#plotting regular Logistic Regression\nplot_decision_regions(X, y, clf = LR, ax=ax[0])\nax[0].set_xlabel('Feature 1')\nax[0].set_ylabel('Feature 2')\nax[0].set_title(f\"Accuracy = {(LR.score(X,y))}\")\n\n#plotting regular Logistic Regression\nplot_decision_regions(X, y, clf = KLR, ax=ax[1])\nax[1].set_xlabel('Feature 1')\nax[1].set_ylabel('Feature 2')\nax[1].set_title(f\"Accuracy = {(KLR.score(X,y))}\")\n\nplt.show()\n\n\n\n\nTherefore, we can clearly see how regular logistic regression, finds the best “straight line” that divides the two classes. However, it is not able to learn the “curvy” nature of the data. On the other hand, kernel logistic regression is able to learn that “curvy” nature, and therefore has a better performance at our classification task.\n\n\nWhat is Kernel Logistic Regression?\nOriginally, when we were dealing with Logistic Regression, our empirical risk minimization problem was:\n\n\\(\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w})\\;\\)\n\nwhere, our loss function \\(L(\\mathbf{w})\\;\\) was defined as:\n\n\\(L(\\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)\\;\\)\n\nNow, in Kernel Logistic Regression, we want to map our original features onto a higher-dimensional space, where it possible to linearly classify our data, for this purpose our empirical risk minimization problem becomes:\n\n\\(L_k(\\mathbf{v}) = \\frac{1}{n} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{v}, \\boldsymbol{\\kappa}(\\mathbf{x}_i) \\rangle, y_i)\\; \\tag{1}\\)\n\nBasically, what we do is we take the rows of the original feature matrix \\(X\\): \\(x_i\\), and perform the mapping \\(\\boldsymbol{\\kappa}(\\mathbf{x}_i)\\) on it. Performing this operation on a feature row \\(x_i\\) results in a modified feature vector \\(\\boldsymbol{\\kappa}(\\mathbf{x}_i)\\), which has entries:\n\n\\(\\boldsymbol{\\kappa}(\\mathbf{x}_i) = \\left( \\begin{matrix}  k(\\mathbf{x}_1, \\mathbf{x}_i) \\\\  k(\\mathbf{x}_2, \\mathbf{x}_i) \\\\  \\vdots \\\\  k(\\mathbf{x}_n, \\mathbf{x}_i) \\end{matrix}\\right)\\;\\)\n\nHere, \\(k(\\mathbf{x}_1, \\mathbf{x}_i)\\) performs the kernel operation between two feature rows, and produces a single-value which for simplicity can be understood as the “similarity” between these two feature rows. Performing this operation on every feature row of the original feature matrix, with respect to every other feature row of the original feature matrix gives us the kernel matrix:\n\n\\(\\boldsymbol{\\kappa}(\\mathbf{X}) = \\left( \\begin{matrix}  k(\\mathbf{x}_1, \\mathbf{x}_1) \\hspace{1cm} k(\\mathbf{x}_1, \\mathbf{x}_2) \\hspace{1cm} k(\\mathbf{x}_1, \\mathbf{x}_3) \\hspace{1cm} k(\\mathbf{x}_1, \\mathbf{x}_n) \\\\  k(\\mathbf{x}_2, \\mathbf{x}_1) \\hspace{1cm} k(\\mathbf{x}_2, \\mathbf{x}_2) \\hspace{1cm} k(\\mathbf{x}_2, \\mathbf{x}_3) \\hspace{1cm} k(\\mathbf{x}_2, \\mathbf{x}_n)\\\\  \\vdots \\\\  k(\\mathbf{x}_n, \\mathbf{x}_1) \\hspace{1cm} k(\\mathbf{x}_n, \\mathbf{x}_2) \\hspace{1cm} k(\\mathbf{x}_n, \\mathbf{x}_3) \\hspace{1cm} k(\\mathbf{x}_n, \\mathbf{x}_n) \\end{matrix}\\right)\\;\\)\n\nTherefore, in the kernel matrix \\(\\boldsymbol{\\kappa}(\\mathbf{X})\\), each column \\(i\\) corresponds to the kernel operation done for the \\(i\\)th row of the original feature matrix, with respect to all feature rows in the original feature matrix. Therefore, we can see that our original feature matrix \\(X \\in \\mathbb{R}^{nxp}\\), becomes the kernel matrix \\(\\boldsymbol{\\kappa}(\\mathbf{X}) \\in \\mathbb{R}^{nxn}\\), with more features we are in a better position to study the data in a higher-dimensional space. However, the fact that our matrix is now \\(nxn\\) means that we need to modify our weight vector as well. Our original feature vector was \\(w \\in \\mathbb{R}^{p}\\), but now our modified weight vector is \\(v \\in \\mathbb{R}^{n}\\). For each column \\(i\\) of the kernel matrix, the prediction is performed as follows:\n\n\\(\\hat{y} = \\langle \\hat{\\mathbf{v}}, \\boldsymbol{\\kappa}(\\mathbf{x_i}) \\rangle\\;\\)\n\nImportant Note: By definition, a regular multiplication of the feature matrix and the weight vector happens along the rows of the matrix, rather than the columns. However, by the properties of a kernel matrix created between the same matrix twice, we are guaranteed symmetry - so we do not need to transpose the matrix. This idea of transposition will become relevant when we perform the prediction function on unseen data (a new feature matrix).\n\n\nImplementation and Explanation of Code\nFor my implementation of Kernel Logistic Regression, these are some of the important functions:\n1. fit function:\ndef fit(self, X, y):\n    self.X_train = X \n    #computing the kernel matrix - which is an NxN matrix \n    km = self.kernel(X, X, **self.kernel_kwargs)\n    #initializing random weight vector - v0\n    v0 = np.random.rand(X.shape[0])\n    #minimize the self.loss function return value by adjusting the v paramater\n    #for starters: take the v parameter as v0\n    result = minimize(lambda v: self.loss(km, y, v), x0 = v0)\n    self.v = result.x\nWe first take the original matrix, and save an instance of it for future use - when we perform predict on unseen data. We then calculate the kernel matrix of the original feature matrix with respect to itself. Now, we can go ahead and initialize a random weight vector \\(v \\in \\mathbb{R}^{n}\\). To find the optimal weight vector, we use the minimize function from scipy.optimize. Here, we pass our loss function - which utilizes logistic loss. 2. predict function:\ndef predict(self, X):\n    km = self.kernel(self.X_train, X, **self.kernel_kwargs)\n    km_transpose = km.T\n    return 1*((km_transpose@self.v)>0)\nThe idea behind Kernel Logistic Regression, when making predictions for unseen data (validation data) is as follows:\n\n\nFor Feature Transformation - creata a Kernel Matrix using the original feature matrix and the new (unseen) feature matrix\n\nFor Predictions - use the optimal weight vector \\(v\\) to make predictions on this Kernel Matrix\n\nTherefore, we first create a kernel matrix from our saved instance of the original feature matrix and the unseen feature matrix. Now, the step where we have to transpose the matrix could have been avoided if we switched the order in which we pass the parameters to the kernel function. However, for this code - the original training feature matrix is \\(X_\\text{training} \\in \\mathbb{R}^{nxp}\\) and the unseen feature matrix is say \\(X_\\text{unseen} \\in \\mathbb{R}^{mxp}\\). Therefore, in the order in which we have passed the parameters, the kernel matrix will be of the order \\(\\mathbb{nxm}\\) - \\(n\\) rows and \\(m\\) columns. Since, the multiplication of the (kernel) matrix and the weight vector \\(v\\) happens along the row, and since in this case our kernel matrix is not symmetrical, we transpose the matrix to make it of the order \\(\\mathbb{R}^{mxn}\\) - \\(m\\) rows and \\(n\\) columns. Now, our weight vector \\(v \\in \\mathbb{R}^n\\) can be multiplied along the row, and we will get \\(m\\) predictions - the number of data points in the unseen feature matrix. 3. RBF Kernel and Gamma: When creating an object of our KernelLogisticRegression class, we did:\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10)\n\n\nrbf_kernel: the type of kernel function which is used while calculating the kernel matrix\n\n\ngamma: the ‘gamma’ parameter controls how the model measures the “similarity” between two things. In simpler words, the gamma parameter basically controls how “curvy” and “squiggly” our lines are. A very small gamma value might lead to underfitting, while a very large gamma value might lead to overfitting\n\n\n\nThe Effect of Gamma\nIn this section, we want to illustrate the effect that the value of gamma has on the training data and the testing data. Generally, speaking:\n\n\nGood Value of Gamma: good score on the training data + does not overfit + is able to generaliez on unseen data\n\nVery Large Value of Gamma: extremely high score on the training data + overfits + fails to generalize on unseen data\n\n\nGood Value of Gamma\nFirst generating and fitting our model on the training data:\n\n#training data\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nNow generating unseen data with similar pattern, and assessing our model’s performance on this unseen data:\n\n#unseen data with similar pattern \nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nThe model performs fairly well on the unseen data. Therefore, we can conclude that a good value of gamma allows for effective learning and generalization on unseen data. Therefore, we do not run into the risk of overfitting.\n\n\nVery Large Value of Gamma\nFirst, generating some training data and fitting our model on this training data:\n\n#training data\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 1000000)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nNow generating unseen data with similar pattern, and assessing our model’s performance on this unseen data:\n\n#unseen data with similar pattern \nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAlthough, a dramatic example it goes on to show us that choosing a very large value of gamma leads to immense overfitting. The decision boundaries become a bunch of “squiggly” lines and the model learns the noise on the training data. Therefore, the model achieves perfect score on the training data. However, when we evaluate this model’s performance on unseen data, it fails to generalize and achieves a really low score on this unseen data.\n\n\n\nFinding the Best Value of Gamma\nNow, let us perform an experiment in which we will find the best value of gamma. To do this, we will evaluate the score of our Kernel Logistic Regression model (with varying values of gamma) on both the training data, and the testing data and look at the results. Since, we are also going to be performing this experiment with varying levels of noise and for different shapes of data, it is useful to construct a function that can easily be called:\n\nnp.random.seed()\ndef performExperiment(dataShape, noise = 0.2, gamma = [-5, 6]):\n    gammaVal = 10.0**np.arange(gamma[0], gamma[1])\n    df = pd.DataFrame({\"gamma\": [], \"train\" : [], \"test\" : []})\n    #finding accuracy over 10 runs\n    for rep in range(10):\n        X_train, y_train = dataShape(100, shuffle = True, noise = noise)\n        X_test, y_test = dataShape(100, shuffle = True, noise = noise)\n        for gamma in gammaVal:\n            KLR = KernelLogisticRegression(rbf_kernel, gamma = gamma)\n            KLR.fit(X_train, y_train)\n            to_add = pd.DataFrame({\"gamma\" : [gamma],\n                                   \"train\" : [KLR.score(X_train, y_train)],\n                                   \"test\" : [KLR.score(X_test, y_test)]})\n            df = pd.concat((df, to_add))\n    \n    means = df.groupby(\"gamma\").mean().reset_index()\n    \n    plt.plot(means[\"gamma\"], means[\"train\"], label = \"training\")\n    plt.plot(means[\"gamma\"], means[\"test\"], label = \"testing\")\n    plt.loglog()\n    plt.legend()\n    labs = plt.gca().set(xlabel = \"Value of Gamma\", ylabel = \"Accuracy (mean over 10 runs)\")\n\nNow, that we have this function that we can call, let us see which value of gamma works best for the moon shape:\n\nperformExperiment(make_moons)\n\n\n\n\nWe can clearly see that an extremely small value of gamma is no good. We can see that the accuracy starts to pick up around gamma = \\(10^{-1}\\). So, for better visualization, we can change our gamma value:\n\nperformExperiment(make_moons, gamma = [-1, 6])\n\n\n\n\nMuch better! From this graph, it seems like the value of gamma that performs the best on the testing data is around gamma = \\(10^{2} = 100\\).\n\n“Noise” and the Best Value of Gamma\nNow, we can experiment with differing noise levels and see the effect that has on the “best” value of gamma. Noise determines how much variability will be in our data, for example in the case of crescents: 0 noise would imply perfect, completely separable crescent shapes made by the two types of data, and 1 noise would imply a lot more overlap.\nFirst for a lower level of noise than the previous experiment:\n\nperformExperiment(make_moons, noise = 0.1, gamma = [-1, 6])\n\n\n\n\nNext with a higher level of noise than the previous experiment:\n\nperformExperiment(make_moons, noise = 0.6, gamma = [-1, 6])\n\n\n\n\nTherefore, we can see that in both cases (although it is more apparent with the experiment with the higher noise), the best value of gamma is still around \\(100\\)! Therefore, we can conclude that the best value of gamma does not depend strongly on the noise of the data. However, there are many other considerations that we have to be wary of. We must note that in different noise levels, the best gamma value, although the same, yields different accuracies. This is because data with lower noise is more easily separable, while data with higher noise is harder to separate!\n\n\n\nDifferent Geometries\nWe can use the make_circles function generate concentric circles, instead of crescents. First, let us visualize what varying levels of “noise” look like in the context of the make_circles function.\nStarting off with an extremely low level of noise:\n\nX, y = make_circles(200, shuffle = True, noise = 0.01)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10)\nKLR.fit(X, y)\n\nplot_decision_regions(X, y, clf = KLR)\n\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nNext, looking at a relatively high level of noise:\n\nX, y = make_circles(200, shuffle = True, noise = 0.4)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10)\nKLR.fit(X, y)\n\nplot_decision_regions(X, y, clf = KLR)\n\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nTherefore, we can clearly see the data with the higher noise has way more variability in where the points fall. Because of this, the Kernel Logistic Regression model performs way better on data with low noise, as compared to data with higher noise. Now, in order to find the best value of gamma for this data, we can repeat the previous experiment but now with circles:\n\nperformExperiment(make_circles, gamma = [-1, 6])\n\n\n\n\nSeems, like gamma=\\(10^{0} = 1\\) performs the best on our testing data!"
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Introduction to Linear Regression\nIn our past blog posts, we have mostly dealt with classification - how do we classify a point as part of one group, or the other. However, in this blog post we are going to be dealing with numerical values - how do we make numerical predictions? This means that given a feature matrix - \\(X \\in \\mathbb{R}^{n*p}\\) - how do we use those features to make numerical predictions about what the value - \\(y_i\\) - will be! In this blog post, we will be dealing with linear regression. This can be visualized as follows:\n\n\n\nBased on the above image, if we were to come up with a prediction of what the output value \\(y_i\\) would be, for each point on the x-axis, \\(x_i\\), how do we do that? In this situation, we are dealing with \\(1\\)-feature - so, how do we predict the output based on that one feature? The goal of linear regression is to come up with a line which most closely simulates the pattern of the data points. One could visualize a line like this, for instance:\n\n\n\nIn this blog post, we will be looking at two ways to find the ideal parameters to formulate the line which closely follows the pattern of the data:\n\n\nUsing an Analytical Formula  When we explicitly set the gradient \\(\\nabla L(w) = 0\\) - condition for minimum! We end up getting the formula, \\(\\hat w = (X^TX)^{-1}X^Ty\\) - from Lecture Notes (03/08). Plugging in all the values, results in an explicit formula that churns out the values for \\(\\hat w\\). However, the problem with this approach is that it is computationally very expensive. Hence, we have a second way! \n\nUsing Gradient Descent  We know that the gradient of the loss function is - \\(\\nabla L(w) = 2X^T(Xw-y)\\) - from Lecture Notes (03/08). Therefore, we can run a loop for a maximum number of times - max_iter - and on each iteration we can make the update \\(w^{(t+1)} \\leftarrow w^{(t)} - (\\alpha * \\text{gradient})\\). We could check for convergence to add a break statement. Precomputing some values allows us to make this computation more efficient\n\n\n\nImplementation of Linear Regression using Analytical Formula\nCreating testing and validation data, to see how our i) Analytical Formula approach, and ii) Gradient Descent approach perform, when we are trying to do linear regression!\n\nimport numpy as np\nfrom matplotlib import pyplot as plt \n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    #creating a random X_train matrix with n_train rows - data points - and p_features columns - the features\n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nGenerating some data using the above method. We are keeping the number of features = \\(1\\), for visualization purposes:\n\nX_train, y_train, X_val, y_val = LR_data(n_train = 100, n_val = 100, p_features = 1, noise = 0.1)\n\n#creating a padded version of X_train and y_train\nX_train_padded = pad(X_train)\nX_val_padded = pad(X_val)\n\nNow let us plot this data:\n\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\nplt.rcParams[\"figure.figsize\"] = (15,5)\naxarr[0].scatter(X_train, y_train, color=\"purple\")\naxarr[1].scatter(X_val, y_val, color=\"purple\")\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\", ylabel = \"y\")\nplt.tight_layout()\n\n\n\n\nUsing our Linear Regression implementation to see how well it performs on the training data and validation data! We will use both analytical formula method, and the gradient descent method.\n\nfrom LinearRegression import LinearRegression\n\n#creating an object of the LinearRegression class\nLR = LinearRegression()\n\n#using the analytical formula first\nLR.fit_analytic(X_train, y_train)\n\nprint(f\"Training score = {LR.score(X_train_padded, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val_padded, y_val).round(4)}\")\n\nTraining score = 0.8754\nValidation score = 0.8974\n\n\nEstimated Weight Vector:\n\nLR.w\n\narray([1.00405436, 0.58991389])\n\n\nWe can visualize the line that the weight vector from our analytical formula forms:\n\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\nplt.rcParams[\"figure.figsize\"] = (15,5)\naxarr[0].scatter(X_train, y_train, color = \"purple\")\nLR.draw_line(axarr[0])\naxarr[1].scatter(X_val, y_val, color = \"purple\")\nLR.draw_line(axarr[1])\nlabs = axarr[0].set(title = f\"Training Score = {LR.score(X_train_padded, y_train).round(4)}\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = f\"Validation Score = {LR.score(X_val_padded, y_val).round(4)}\", xlabel = \"x\", ylabel = \"y\")\nplt.tight_layout()\n\n\n\n\n\n\nImplementation of Linear Regression using Gradient Descent\n\nLR2 = LinearRegression()\nLR2.fit_gradient(X_train, y_train, alpha = 0.0001, max_iter = 10000)\n\n#using the gradient descent method \nprint(f\"Training score = {LR2.score(X_train_padded, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val_padded, y_val).round(4)}\")\n\nTraining score = 0.8754\nValidation score = 0.8974\n\n\nEstimated Weight Vector:\n\nLR2.w\n\narray([1.00405322, 0.5899145 ])\n\n\nWe can visualize the line that the weight vector from our gradient descent method forms:\n\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\nplt.rcParams[\"figure.figsize\"] = (15,5)\naxarr[0].scatter(X_train, y_train, color = \"purple\")\nLR2.draw_line(axarr[0])\naxarr[1].scatter(X_val, y_val, color = \"purple\")\nLR2.draw_line(axarr[1])\nlabs = axarr[0].set(title = f\"Training Score = {LR2.score(X_train_padded, y_train).round(4)}\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = f\"Validation Score = {LR2.score(X_val_padded, y_val).round(4)}\", xlabel = \"x\", ylabel = \"y\")\nplt.tight_layout()\n\n\n\n\nThe analytical way gives us the weight vector \\(\\hat w\\), which is optimal - mathematically. While, the gradient descent method, iteratively tries to find this optimal solution. However, since our learning rate - \\(\\alpha\\) - is small enough, and our maximum number of iterations is large enough, both these approaches give us similar results. This is because the slow learning rate and large number of iterations, truly allow the gradient descent algorithm to find the minimum of the loss function.\n\n\nVisualizing the Increase of Score over time (for Gradient Descent method)\nAs mentioned in the instructions for this blog post, the score should increase monotonically in each iteration. Let us see if that is true:\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nBased on the graph, that is indeed true. Therefore, we can be assured that our implementation of Gradient Descent for Linear Regression is working properly.\n\n\nHow does Increase in the Number of Features Affect the Score?\nSo far, for the sake of easy visualization, we have been dealing with a single feature linear regression. That is, we have one value of \\(X\\) - that is \\(X\\) is a \\(n\\)x\\(1\\) matrix, and we produce a single numerical output. However, how does an increase in the number of features affect the overall score of our linear regression algorithm? For this purpose, first let us produce data with increasing number of features, and then plot the number of features on the \\(x\\)-axis, and the overall score on the \\(y\\)-axis. In this blog post, I will find for both: the i) analytical way, and the ii) gradient descent way, how the increase in the number of features affects the overall score on the training data set and the validation data set.\n\nLR_exp = LinearRegression()\ntrain_score = []\nval_score = []\nfor i in range(1, n_train):\n    X_train, y_train, X_val, y_val = LR_data(n_train = 100, n_val = 100, p_features = i, noise = 0.1)\n    LR_exp.fit_analytic(X_train, y_train)\n    train_score.append(LR_exp.score(pad(X_train), y_train))\n    val_score.append(LR_exp.score(pad(X_val), y_val))\n\nNow that we have our data, let us plot it, to see how the score for the test data and for the validation data changes as the number of features increase:\n\nplt.plot(train_score, label = \"Training\", color = \"red\")\nplt.plot(val_score, label = \"Validation\", color = \"green\")\nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\nplt.legend()\nplt.show()\n\n\n\n\nNow, let us run the same experiment, but this time we will use the  gradient descent method :\n\nLR_exp = LinearRegression()\ntrain_score = []\nval_score = []\nfor i in range(1, n_train):\n    X_train, y_train, X_val, y_val = LR_data(n_train = 100, n_val = 100, p_features = i, noise = 0.1)\n    LR_exp.fit_gradient(X_train, y_train, alpha = 0.0001, max_iter = 10000)\n    train_score.append(LR_exp.score(pad(X_train), y_train))\n    val_score.append(LR_exp.score(pad(X_val), y_val))\n\nNow that we have our data, let us plot it, to see how the score for the test data and for the validation data changes as the number of features increase:\n\nplt.plot(train_score, label = \"Training\", color = \"red\")\nplt.plot(val_score, label = \"Validation\", color = \"green\")\nlabels = plt.gca().set(xlabel = \"Number of Features\", ylabel = \"Score\")\nplt.legend()\nplt.show()\n\n\n\n\nTherefore, we can see that in both cases: as the number of features keeps on increasing, the training score converges (plateaus) towards a perfect score, however the validaion score starts decreasing after a certain number of features. This is because as the number of features goes up, we start accomodating for noise and therefore - overfitting. One difference, between both the cases is that the analytical method to get the weight vector, seems to be doing relatively better on the validation data as the number of features goes up, compared to the gradient descent method, which struggles to keep up its validation score from early on.\n\n\nLasso Regularization\nHow do we combat overfitting? Regularization. In this part of the blog post, we will implement what is called Lasso Regularization. Using this regularization pushes down the values of the weight vector \\(w\\), this helps us to reduce overfitting. The Lasso Regularization is defined as (check link for source): \\[L(\\mathbf{w}) = \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2 + \\alpha \\lVert \\mathbf{w}' \\rVert_1\\;\\]\nWhere \\(\\alpha\\) is the term which controls the strength of the regularization. Now, let us implement Lasso Regularization with varying strengths of the regularization. While doing so, let us see how the increase of the number of features affects the training score and the validation score:\n\nfrom sklearn.linear_model import Lasso\n\ntrain_score_1 = []\nval_score_1 = []\ntrain_score_2  = []\nval_score_2 = []\ntrain_score_3 = []\nval_score_3 = []\n\n#initializing the Lasso Objects\nL1 = Lasso(alpha=0.1)\nL2 = Lasso(alpha=0.01)\nL3 = Lasso(alpha=0.001)\n\n\nfor i in range(1, n_train):\n    X_train, y_train, X_val, y_val = LR_data(n_train = 100, n_val = 100, p_features = i, noise = 0.1)\n    #first object\n    L1.fit(X_train, y_train)\n    train_score_1.append(L1.score(X_train, y_train))\n    val_score_1.append(L1.score(X_val, y_val))\n    #second object\n    L2.fit(X_train, y_train)\n    train_score_2.append(L2.score(X_train, y_train))\n    val_score_2.append(L2.score(X_val, y_val))\n    #third object\n    L3.fit(X_train, y_train)\n    train_score_3.append(L3.score(X_train, y_train))\n    val_score_3.append(L3.score(X_val, y_val))\n\nNow that we have our data, let us plot it:\n\nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True)\nplt.rcParams[\"figure.figsize\"] = (15,5)\n\n#first object\naxarr[0].plot(train_score_1, label = \"Training\", color = \"red\")\naxarr[0].plot(val_score_1, label = \"Validation\", color = \"green\")\naxarr[0].legend()\n#second object\naxarr[1].plot(train_score_2, label = \"Training\", color = \"red\")\naxarr[1].plot(val_score_2, label = \"Validation\", color = \"green\")\naxarr[1].legend()\n#third object\naxarr[2].plot(train_score_3, label = \"Training\", color = \"red\")\naxarr[2].plot(val_score_3, label = \"Validation\", color = \"green\")\naxarr[2].legend()\n\n#setting labels and titles\nlabels = axarr[0].set(title = \"Alpha = 0.1\", xlabel = \"Number of Features\", ylabel = \"Score\")\nlabels = axarr[1].set(title = \"Alpha = 0.01\", xlabel = \"Number of Features\", ylabel = \"Score\")\nlabels = axarr[2].set(title = \"Alpha = 0.001\", xlabel = \"Number of Features\", ylabel = \"Score\")\n\nplt.show()\n\n\n\n\nNow there are a couple of things to unpack here:\n\n\nWhen the size of \\(\\alpha\\) is large: in the first case, when the size of \\(\\alpha = 0.1\\), we can see how the regularization affects the performance. We are unable to find the optimal weight vector due to this heavy regularization. And between \\(\\alpha = 0.01\\) and \\(\\alpha = 0.001\\), we can see that the latter leads to better performance and convergence. Therefore, the size of \\(\\alpha\\) has to be small enough, so that it does not affect our finding of the optimal weight vector. However, it also has to be large enough to have a significant regularizing effect.\n\nComparison to Standard Linear Regression: When the value of \\(\\alpha\\) is appropriate, the Lasso method seems to lead to faster convergence to accuracy. However, we can again notice the effects of “overfitting” on the validation data. However, this data is surprising to me as the Lasso method does not seem to be doing any better than the Standard Linear Regression when it comes to the validation data, since the whole purpose of the regularization is to avoid overfitting and to get better results on the validation data.\n\n\n\nAdditional Experiment: Time Complexity of Analytical Formula and Gradient Descent Method\nIn this additional section of the blog post, let us try to understand how the increase of the n_train - the number of training data points - affects the time that our two methods take to find the optimal weight vector \\(w\\). For this purpose, we would be experimenting on a bunch of values of n_train, and then computing the time that both the approaches took. Then we will plot the time that both the approaches took versus the number of data points:\n\nimport time\n\nLR_time = LinearRegression()\ntrain_time_analytical = []\ntrain_time_gradient = []\nfor i in range(100, 5000):\n    X_train, y_train, X_val, y_val = LR_data(n_train = i, n_val = i, p_features = 1, noise = 0.1)\n    \n    start_time = time.time()\n    LR_time.fit_analytic(X_train, y_train)\n    end_time = time.time()\n    train_time_analytical.append(end_time - start_time)\n    \n    start_time = time.time()\n    LR_time.fit_gradient(X_train, y_train, alpha = 0.0001, max_iter = 10000)\n    end_time = time.time()\n    train_time_gradient.append(end_time - start_time)\n    \n\nNow that we have the data, let us plot to see it:\n\nplt.plot(np.arange(100, 5000), train_time_analytical, label = \"Analytical Method\", color = \"red\")\nplt.plot(np.arange(100, 5000), train_time_gradient, label = \"Gradient Descent Method\", color = \"green\")\nlabels = plt.gca().set(xlabel = \"Number of Data Points\", ylabel = \"Time Taken (s)\")\nplt.legend()\nplt.show()\n\n\n\n\nWe can see that the analytical method remains at a constant speed for most of our data points. But, we should also keep in mind that is only going till \\(5000\\) data points (limited by the technical capabilities of this laptop), and in real life all data points are way larger. In that situation the analytical will take a lot of time, as determined by the complexity analysis of the analytical method. This will also be true for the gradient descent method, however its time complexity is lower than the analytical method, so it will take less time. Based on this particular example though, we can see that the time taken by gradient descent kept on decreasing from \\(100\\) data points to \\(5000\\) data points. Upon further research, I found out that this happens because an increased number of data points lead to more stable gradient values - of the loss function - and therefore faster convergence."
  },
  {
    "objectID": "posts/perceptron-algorithm/index.html",
    "href": "posts/perceptron-algorithm/index.html",
    "title": "Perceptron Algorithm for Classification",
    "section": "",
    "text": "Introduction to the Perceptron Algorithm\nThe Perceptron Algorithm is a linear classifier, which means that based on an object’s properties it should be able to classify it as part of one group or another. This could be extended to a general problem. Given two features: \\(x_1\\) - a person’s salary, and \\(x_2\\) - their age, how likely are they to buy a car? We can store, the two features in a single vector: \\(x_i = (x_1, x_2) \\in R^2\\). Alternatively, we can create a feature matrix \\(X\\) and stack \\(x_1\\) and \\(x_2\\) onto that matrix.\n\\[\nX =\n\\begin{bmatrix}\nx_1\\\\\nx_2\n\\end{bmatrix}\n\\]\nFor \\(n\\) data points and \\(p\\) features, the feature matrix \\(X\\) will be an \\(n\\) x \\(p\\) matrix. To find a linear classifier, we start with a training set, where have access to the lablels - which can be \\(\\{+1,-1\\}\\). Building up on the example of buying a car, \\(+1\\) could mean the person is likely to buy a car, and \\(-1\\) could mean the opposite. We start with bias as \\(b=0\\) and initialize random weights for each feature. We then find the weighted sum - which is called the activation - \\(a\\). Based on whether this activation (the activation’s sign) matches with the label (the label’s sign), we update the weights and bias. We have some iterations of the perceptron algorithm over the test data, the goal being to be able to correctly guess labels for the test data which the model has not seen before. Visually - for two features - we could imagine the two axes of a cartesian plane as the two features. Our goal is to find a hyperplane that can divide the data (which would be in the form of points) into two groups.\n\n\nImplementing the Perceptron Algorithm\n\n#importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron \n\nIn the following section, we are using the make_blobs function of sklearn.datasets to create synthetic data for our experimentation. In this case, we are creating two clusters of data, which are centered around two different areas - and have different labels \\(0\\) and \\(1\\). This synthetic data will be used to train our perceptron algorithm.\n\n#arbitrarily setting the seed number to 12345 to ensure reproducibility\nnp.random.seed(12345)\n\n#setting the number of points we want\nn = 100\n#setting the number of features = 2 for visualization purposes\nfeatures = 2\n\n#using the make_blobs function to create 2 clusters of 100 data points\n#X is the feature matrix - in this case n x features i.e. 100 x 2 \n#y is the cluster labels - whether Alice ikes a book or not - in this case 0 or 1, instead of -1 and +1\nX, y = make_blobs(n_samples = n, n_features = features, centers = [(-1.7,-1.7),(1.7,1.7)])\n\n\n#plotting the clusters generated by the make_blobs function\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxLabel = plt.xlabel(\"Feature 1\")\nyLabel = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn the following section, we are calling the fit function of the Perceptron class. This function runs for a specified number of times, which in our case is max_steps. The function stops running either when we have reached the specified number of times, or when we achieve perfect accuracy in our classification. In each run, the dot product of the weight vector (which is randomly initialized) and the feature vector for a particular data point, is calculated. This is our predicted label. Based on how this predicted label compares to the actual label - we either update the weight vector, or leave it unchanged.\nUpdate Mechanism of the Perceptron algorithm: in the case where the predicted label does not match the actual label, the program modifies the weight vector! This can be explained as follow:\nif(yPrediction!=y[index]):\n    yModified = 2*y[index] - 1\n    self.w = np.add(self.w, yModified*X_[index,:])\nWe check for the condition in which the prediction label does not match the actual label. Then the idea is for the weight vector: \\(w = \\{w_1, w_2, -b\\}\\), we need to create a new weight vector \\(w^{'} = \\{w_1 + y_i * x_{i1}, w_2 + y_i * x_{i2}, -b + y_i\\}\\). In our case, since we are dealing with \\(0\\)s and \\(1\\)s, rather than the conventional \\(+1\\)s and \\(-1\\)s, we need to modify the \\(y_i\\). For this purpose, a yModified variable is created which is \\((2 * y_{i}) - 1\\). Therefore, \\(1\\) gets mapped to \\(1\\), and \\(0\\) gets mapped to \\(-1\\). This is necessary because when using \\(0\\) as a label, we run into the issue where \\(0\\) leaves \\(w^{'} = \\{w_1 + y_i * x_{i1}, w_2 + y_i * x_{i2}, -b + y_i\\}\\) unchanged, as \\(\\{y_i * x_{i1}, y_i * x_{i2}, y_i\\}\\) are all \\(0\\).\n\n#creating an instance of the Perceptron class\np = Perceptron()\n#max_steps - number of times we will go over the dataset\np.fit(X, y, max_steps = 1500)\n\nIn the following section, we can see the final weight vector we achieved - which in this case led to perfect accuracy. This can be verified by looking at the history variable, which shows that a score of 1.0 was achieved at the end!\n\nprint(p.w)\nprint(p.history[-10:]) \n\n[ 1.71328294  2.81366675 -0.50209446]\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\nA visualization of how the accuracy of the classifier increases over the number of iteration. Using the score function, we can also see the final score, which in this case was 1.0 - perfect classification!\n\np.visualize_score()\nprint(p.score(X,y))\n\n1.0\n\n\n\n\n\nWe can visualize the line which classifies the two objects:\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = p.draw_line(p.w, -4, 4)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nPerceptron Algorithm on Data which is not Linearly Separable\nWhen the data points are not linearly separable the Perceptron algorithm will not be able to settle on a weight vector - \\(w\\) - which perfectly divides the data points into two regions. In theoretical terms this means that based on the \\(2\\) features, the Perceptron algorithm is not able to come up with a weight vector \\(w\\), which can predict with \\(100%\\) accuracy, whether an object belongs to one group or another. For example - building up on our original example - based on a person’s \\((1)\\) age, and \\((2)\\) salary, the Perceptron algorithm is not able to predict whether this person will buy a car or not.\nIn order to visualize data which is not linearly separable, we will follow the same steps as before, the only change would be in the line:\n    X, y = make_blobs(n_samples = n, n_features = features, centers = [(-1.7,-1.7),(1.7,1.7)])\nWe can change the centers of the two clusterings, to make them have overlaps. Something like:\n    X, y = make_blobs(n_samples = n, n_features = features, centers = [(0.5,0.5),(1.7,1.7)])\n\n#importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron \n\nnp.random.seed(12345)\n\nn = 100\nfeatures = 2\n\nX, y = make_blobs(n_samples = n, n_features = features, centers = [(0.5,0.5),(1.7,1.7)])\n\n#plotting the clusters generated by the make_blobs function\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxLabel = plt.xlabel(\"Feature 1\")\nyLabel = plt.ylabel(\"Feature 2\")\n\n\n\n\nTherefore, we can clearly see that the two types of data (objects) have more overlap now, and are not linearly separable. That is, you cannot imagine a straight line which divides the data points into two regions perfectly.\nIn this situation, since the Perceptron algorithm will never reach the situation in which the score is 1.0 - it will stop running once it reaches the specified number of max_steps. This experiment will seek to explore what happens as the number of max_steps is increased. First, let us start with the same magnitude of max_steps as the previos linearly separable case, that is: max_steps = 1500\n\n#creating an instance of the Perceptron class\np = Perceptron()\n#max_steps - number of times we will go over the dataset\np.fit(X, y, max_steps = 1500)\n\nClearly, we can see that the maximum score that we are able to achieve is 0.74! Furthermore, we can see that our accuracy was better off in the beginning, when the weight vector \\(w\\) was almost random.\n\nprint(p.history[-10:]) \np.visualize_score()\nprint(p.score(X,y))\n\n[0.68, 0.68, 0.68, 0.68, 0.68, 0.68, 0.74, 0.74, 0.74, 0.74]\n0.74\n\n\n\n\n\nEven in the final line we achieve, we can see that it does not perfectly separate the data into two regions!\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = p.draw_line(p.w, -4, 4)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nNow, let us try to increase the max_steps to an aribtrarily large number, say max_steps=50000\n\np_new = Perceptron()\np_new.fit(X, y, max_steps=50000)\n\nClearly, we can see that the maximum score we achieve is 0.76!\n\nprint(p_new.history[-10:]) \np_new.visualize_score()\nprint(p_new.score(X,y))\n\n[0.5, 0.72, 0.72, 0.77, 0.67, 0.77, 0.77, 0.69, 0.76, 0.76]\n0.76\n\n\n\n\n\nEven in the final line we achieve, we can see that it does not perfectly separate the data into two regions! By running for \\(50,000\\) data points instead of \\(1500\\), our score did not change a lot. Therefore, we can tell that if the data is not linearly separable - the perceptron does not converege!\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = p_new.draw_line(p_new.w, -4, 4)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nPerceptron Algorithm in p = 5 Dimension (5 Features)\nIn the case when we were working with \\(p=2\\) features (that is, \\(2\\)-Dimensions), the classifier was a straight line (that is, \\(1\\)-Dimensional). In the situation where we are working with \\(p=5\\) features, the classifier will be a \\(4\\)-Dimensional hyerplane.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\nnp.random.seed(12345)\n\nn = 100\nfeatures = 5\n\nX, y = make_blobs(n_samples = n, n_features = features)\n\nIn the above code, the make_blobs function creates \\(100\\) data points, with \\(5\\) features each. Because of this we can say that, \\(X\\) is a \\(100\\) x \\(5\\) matrix. Since we have not specified the centers, make_blobs will automatically create \\(3\\) clusters. In this case, \\(y=\\{0,1,2\\}\\). In this case, given that we are dealing with \\(5\\) features, we have to explicitly pass the following parameter to the constructor:\n    pFiveD = Perceptron(w = np.random.rand(6))\nThis is because in our original case, since features=2 our weight vector \\(w\\) had 3 elements, one weight for each feature, plus a bias term. Since, now we are dealing with \\(5\\) features, we need \\(6\\) terms in our weight vector.\n\n#creating an instance of the Perceptron class\npFiveD = Perceptron(w=np.random.rand(6))\n\nStarting with max_steps=1500\n\n#setting max-steps the same as the previous experiments\npFiveD.fit(X, y, max_steps = 1500)\n\nWe can see that the max score we are able to obtain is \\(0.32\\)!\n\nprint(pFiveD.history[-10:])\npFiveD.visualize_score()\nprint(pFiveD.score(X,y))\n\n[0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32]\n0.32\n\n\n\n\n\nTrying the same fit, but with max_steps=50000:\n\npFiveD = Perceptron(w=np.random.rand(6))\npFiveD.fit(X, y, max_steps = 50000)\n\nWe can see that the max score we are able to obtain is \\(0.32\\)!\n\nprint(pFiveD.history[-10:])\npFiveD.visualize_score()\nprint(pFiveD.score(X,y))\n\n[0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32, 0.32]\n0.32\n\n\n\n\n\nSince the maximum score, we are able to achieve is \\(0.32\\), I believe that the data is not linearly separable!\n\n\nRuntime Complexity of a Single Iteration of the Perceptron algorithm update\nThe runtime complexity of a single iteration of the Perceptron algorithm update depends on the number of features of the data - \\(p\\), but not on the number of data points - \\(n\\). Specifically, the runtime complexity is proportional to the number of features - \\(p\\). This is because for each feature, we need to multiply its value by the corresponding weight and then sum these products together.\nIf there are \\(p\\) features, the runtime complexity of a single iteration of the Perceptron algorithm update is \\(O(p)\\)!\nThe runtime complexity of a single update does not depend on the number of data points because the perceptron algorithm updates the weights one data point at a time!"
  },
  {
    "objectID": "posts/perceptron-proof/index.html",
    "href": "posts/perceptron-proof/index.html",
    "title": "Mathematical Proof of the Perceptron Learning Rule",
    "section": "",
    "text": "The Perceptron Learning Rule is based on an update which only happens when the activation (related to the predicted label) is different from the actual label. In the situation where the activation and the actual label are the same, no update takes places!\nThe learning rule is based on the following mathematical formula: \\[\nw_{d} = w_{d} + yx{_d}\n\\] \\[\nb = b + y\n\\] where y is the actual label value!\nWe have current parameters, \\[\nw = (w_1, w_2, w_3,....., w_d)\n\\] \\[\n\\text{where  } w \\in \\mathbb{R}^{d}\n\\] \\[\nb \\rightarrow \\text{bias}\n\\]\nFor an example: \\((x, y)\\) where \\(x \\in \\mathbb{R}^{d}\\) and \\(d\\) = number of features, and \\(y = \\pm 1\\) is the true label!\n\n\nLet us say, an example is of a positive label: \\(y = +1\\), but our activation is wrong, \\(a < 0\\)!\nUpdating the weights and bias, we have a modified weight vector \\(w^{'}\\) and a modified bias \\(b^{'}\\): \\(w^{'} = (w_{1}^{'}, w_{2}^{'}, w_{3}^{'},......, w_{d}^{'})\\) and \\(b^{'}\\)\nWhen we observe the same example again, we compute the new activation \\(a^{'}\\):\n\\[\na^{'} = \\sum_{d = 1}^{D}w_{d}^{'}x_{d} + b^{'} = \\sum_{d = 1}^{D}(w_{d} + yx_{d})x_{d} + (b + y)\n\\]\nSince, \\(y = +1\\): \\[\na^{'} = \\sum_{d = 1}^{D}(w_{d} + x_{d})x_{d} + (b + 1)\n\\] \\[\n= \\sum_{d = 1}^{D}w_{d}x_{d} + \\sum_{d = 1}^{D}(x_{d})^{2} + b + 1\n\\] \\[\n= [\\sum_{d = 1}^{D}w_{d}x_{d} + b] + \\sum_{d = 1}^{D}(x_{d})^{2} + 1\n\\]\nSince, \\(a \\text{ (the old activation)}\\) = \\(\\sum_{d = 1}^{D}w_{d}x_{d} + b\\), we have: \\[\na^{'} = a + \\sum_{d = 1}^{D}(x_{d})^{2} + 1 \\text{ which makes } a^{'} > a\n\\] Since, \\((x_d)^{2} \\geq 0\\), \\(a^{'}\\) is always at least 1 more than \\(a\\). Since our activation a was \\(a < 0\\), and \\(y\\) was \\(y = +1\\), we have successfully moved \\(a^{'}\\) in the right direction i.e. towards positive!\n\n\n\nLet us say, an example is of a positive label: \\(y = -1\\), but our activation is wrong, \\(a > 0\\)!\nUpdating the weights and bias, we have a modified weight vector \\(w^{'}\\) and a modified bias \\(b^{'}\\): \\(w^{'} = (w_{1}^{'}, w_{2}^{'}, w_{3}^{'},......, w_{d}^{'})\\) and \\(b^{'}\\)\nWhen we observe the same example again, we compute the new activation \\(a^{'}\\):\n\\[\na^{'} = \\sum_{d = 1}^{D}w_{d}^{'}x_{d} + b^{'} = \\sum_{d = 1}^{D}(w_{d} + yx_{d})x_{d} + (b + y)\n\\]\nSince, \\(y = -1\\): \\[\na^{'} = \\sum_{d = 1}^{D}(w_{d} - x_{d})x_{d} + (b - 1)\n\\] \\[\n= \\sum_{d = 1}^{D}w_{d}x_{d} - \\sum_{d = 1}^{D}(x_{d})^{2} + b - 1\n\\] \\[\n= [\\sum_{d = 1}^{D}w_{d}x_{d} + b] - [\\sum_{d = 1}^{D}(x_{d})^{2} + 1]\n\\]\nSince, \\(a \\text{ (the old activation)}\\) = \\(\\sum_{d = 1}^{D}w_{d}x_{d} + b\\), we have: \\[\na^{'} = a - [\\sum_{d = 1}^{D}(x_{d})^{2} + 1] \\text{ which makes } a^{'} < a\n\\] Since, \\((x_d)^{2} \\geq 0\\), \\(a^{'}\\) is always at least 1 less than \\(a\\). Since our activation a was \\(a > 0\\), and \\(y\\) was \\(y = -1\\), we have successfully moved \\(a^{'}\\) in the right direction i.e. towards negative!"
  },
  {
    "objectID": "posts/pwnable-kr-ctfs/index.html",
    "href": "posts/pwnable-kr-ctfs/index.html",
    "title": "PWNABLE KR (CTF)",
    "section": "",
    "text": "These solutions pertain to the CTF challenges available on pwnable.kr!"
  },
  {
    "objectID": "posts/pwnable-kr-ctfs/index.html#toddlers-bottle",
    "href": "posts/pwnable-kr-ctfs/index.html#toddlers-bottle",
    "title": "PWNABLE KR (CTF)",
    "section": "Toddler’s Bottle",
    "text": "Toddler’s Bottle\n\nfd\nWe first run the ls command to see what we are working with. We have three files:\n\nfd\nfd.c\nflag\n\nAs expected, the flag file does not have read permissions for us. So, let’s examine the code. We can run the following command to output the contents of the fd.c file:\ncat fd.c\nUpon examining the contents of the program, it is clear that we need a way to fill the string “LETMEWIN\\n” into the buf. How? When we examine the read syscall closely, we see that it reads in from the file descriptor stored in int fd, which is essentially argv[1] converted to an int minus 0x1234. Now, we need to understand the following building-blocks:\n\nSTDIN is 0: If we manage to set int fd to 0, it will allow us to write directly from the terminal. This means whatever we type into the terminal will be entered into buf, which is great because we can type “LETMEWIN\\n”.\n\nHowever, we see that:\nint fd = atoi(argv[1]) - 0x1234;\nTo see what value 0x1234 (hex) is in decimal, we can simply type this into the shell:\nprintf \"%d\\n\" 0x1234\nOutput: 4660\nSo, to set fd=0, we have to pass in 4660 as the input. We run the program:\n./fd 4660\nand voila, we are allowed to enter a string. Now we can simply write “LETMEWIN\\n” (without the quotes, and \\n meaning an actual press of the return key), and behold the flag is:\nflag: mommy! I think I know what a file descriptor is!!\nQuestion: How are we not able to use cat flag to output the contents due to permission issues, but when we ourselves run this fd program with the correct input, we are able to access the contents of the file? After all, the process fd was run by us non-privileged users.\nAnswer: If you run ls -l and look at the permissions, you will notice an s in the owner’s execute permissions. Something like: -r-sr-x---. This is called the setuid bit, and any program with the setuid bit set will run with the permissions of the owner. Therefore, when we correctly execute the program, it is able to print the content of the file “flag” as it is running with the permissions of the owner of “fd” file - who also happens to be the owner of the “flag” file. This happens instead of “fd” running with our permissions.\n\n\ncollision\nIn this challenge, we need to use the col program to output the contents of the flag file, despite not having ‘read’ permission. To achieve this, we need to understand how the program works, especially the check_password function.\n\nAnalyzing the col.c Program\n High-Level Overview  1. Command-Line Argument: we pass a command-line argument, which acts as the ‘password’. 2. Password Length: the password must be exactly 20 bytes long. 3. Password Check: the password is passed to the check_password, which verifies if it matches 0x21DD09EC.\n\n\nUnderstanding the Target Hash\nThe check_password function returns a long vlaue. To understand our target, let us convert the hexadecimal value 0x21DD09EC to its decimal representation:\nprintf \"%d\\n\" 0x21DD09EC\nOutput: 568134124\n check_password function  The function takes a string input (a char pointer) and casts it into an int pointer. Here is how it processes the input: 1. Pointer Evaluation: the function evaluates the pointer 4 bytes at a time, doing this 5 times (20 bytes total). 2. Integer Addition: reads the first 4 bytes, interprets them as an int, adds this value to res.\nCode Explanation: Here’s the relevant code snippet:\nint* ip = (int*)p;\nint i;\nint res;\nfor(i = 0; i < 5; i++) {\n    res += ip[i];\n}\nThis code essentially finds different indices of the int pointer and adds the value that is pointed to at that memory address. An alternative approach could be:\nint* ip = (int*)p;\nint i;\nint res;\nfor(i = 0; i < 5; ip++) {\n    res += *ip;\n}\nIn this version, since ip is an int pointer, ip++ will automatically increase the value by 4 bytes, checking 5 integer values.\n\n\nCrafting the Solution\nWe need to pass values in the form of a string that, when evaluated as 5 ints, add up to 568134124. Calculating Integer Values First, let us divide 568134124 by 5: \\(\\frac{568134124}{5} = 113626824.8\\)\nSince we cannot divide evenly, we adjust as follows: 1. int 1: 113626824 2. Remaining Value: \\(568134124\\) - \\(113626824\\) = \\(454507300\\) 3. Dividing Remaining Value: \\(\\frac{454507300}{4} = 113626825\\)\nThus, our integers are:  int 1: 113626824  int 2: 113626825  int 3: 113626825  int 4: 113626825  int 5: 113626825\nConvertion to Hexadecimal We have to convert these integes to hexadecimal:  int 1: 113626824 = 0x6C5CEC8  int 2 - 5: 113626825 = 0x6C5CEC9\nTo make this exactly 4 bytes, we add a leading zero:  0x06C5CEC8 and 0x06C5CEC9\nImplementing the Solution We use perl to construct the input string:\n./col $(perl -e 'print \"\\x06\\xC5\\xCE\\xC9\" x 4 . \"\\x06\\xC5\\xCE\\xC8\"')\nThis should convert to ints that add up to 568134124. However, due to little-endian architecture, we need to reverse the byte order:\n./col $(perl -e 'print \"\\xC9\\xCE\\xC5\\x06\" x 4 . \"\\xC8\\xCE\\xC5\\x06\"')\n\n\nFlag\nflag: daddy! I just managed to create a hash collision :)\n\n\n\nbof"
  },
  {
    "objectID": "posts/renaming-files-ufs2/index.html",
    "href": "posts/renaming-files-ufs2/index.html",
    "title": "Renaming Files in UFS2 : The Hard Way",
    "section": "",
    "text": "Github: dcruzeneil/rename-command\nThe UFS2 (Unix File System) is supported by many Unix-based operating systems. In this project, I wrote a C program to traverse the structures used by UFS2 to represent files and directories, focusing on renaming files. This process provided valuable insights into how UFS2 manages file names, permissions, file contents, and their representation on the disk.\nThis blog post is not a comprehensive breakdown of UFS2, but it covers some key concepts:\n\n\nSuperblock: This structure contains metadata about the entire file system, such as fragment size and the location of the root inode.\n\nDirectory Structures: Each directory structure points to an inode (except the root directory, which just has an inode). It contains the inode number, file/directory name, and file type (directory, file, or other).\n\nInode: Each file and folder has an associated inode that stores permissions and other metadata. For folders, the inode’s data blocks point to other directory structures for the files/folders within that inode. For files, the data blocks point to the actual data.\n\nData blocks refer to 32K chunks of memory on the disk.\n\n\nBefore renaming the files using my command, I ran my implementation of fs-find to list all the files in the raw image of a disk partition. Here is the output:\n\n\n\n Next, I ran the rename command:\n\n\n\n Finally, here is the output of fs-find after executing the rename command:"
  },
  {
    "objectID": "posts/unsupervised-learning/index.html",
    "href": "posts/unsupervised-learning/index.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "Matrix Approximation using SVD\nIn this section of the blog post, we aim to:\n\n\nGet acquainted with numpy’s implementation of SVD\n\nDevelop an appreciation of how SVD allows us to approximate a matrix \\(A\\) using much smaller representations of matrices\n\n\nGetting Acquainted with Numpy’s Implementation of SVD\nBefore we decompose a matrix \\(A\\) using SVD, we need to construct a sample matrix \\(A\\):\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12345)\n\na1 = np.random.randint(1, 3, (5, 4))\na2 = np.random.randint(1, 3, (4, 8))\n\n#constructing our sample matrix A\nA = a1@a2 + 0.1*np.random.rand(5, 8)\n\nNow, we can go ahead and visualize this matrix:\n\nA\n\narray([[11.09940146, 11.06768737, 12.07908225, 14.01709143, 14.00268493,\n        11.08003702, 10.09037225, 11.00246762],\n       [ 7.04917473,  8.05262552,  9.0596366 , 10.00519575, 10.08950895,\n         8.07282662,  7.081835  ,  8.05002228],\n       [11.08101894, 10.00959685, 12.021895  , 14.02587191, 14.04681058,\n        10.04593732, 11.07095098, 10.0178053 ],\n       [ 9.05314499, 10.01677422, 11.07688139, 12.09281705, 12.06094937,\n         9.01501835,  8.04896267, 10.0377345 ],\n       [10.08486014, 11.09110972, 13.03838487, 14.03154959, 14.05683942,\n        10.0187818 , 10.01258415, 11.06875958]])\n\n\nWith matrices, it is often of utility to visualize them as an image to get an understanding of the underlying pattern:\n\nplt.imshow(A, cmap = \"Greys\")\na = plt.gca().axis(\"off\")\n\n\n\n\nNow that we have our matrix \\(A\\), we can use numpy’s implementation of SVD, to get \\(U\\), \\(V\\), and sigma which has been discussed above. The only difference is that we get sigma and not \\(D\\). This means that we get a numpy array of the singular values of \\(A\\), but we have to construct a diagonal matrix - \\(D\\) - ourselves, where the diagonals contain the singular values of \\(A\\). We can wrap up all of these functionalities in a single getValues function, which will call the diagonalize function to create the matrix \\(D\\) out of sigma:\n\ndef getValues(matrix):\n    U, sigma, V = np.linalg.svd(matrix)\n    D = diagonalize(matrix, sigma)\n    return U, D, V\n\ndef diagonalize(matrix, sigma):\n    # creating the D matrix in the SVD \n    # here the matrix of 0s will have the same shape as A\n    D = np.zeros_like(matrix, dtype=float)\n    # putting the singular values along the diagonal of D\n    D[:min(matrix.shape),:min(matrix.shape)] = np.diag(sigma)  \n    return D\n\nU, D, V = getValues(A)\n\nNow, let us try to reconstruct \\(A\\) using the decomposed matrices as displayed in the formula above and compare it to the original \\(A\\):\n\nU @ D @ V\n\narray([[11.09940146, 11.06768737, 12.07908225, 14.01709143, 14.00268493,\n        11.08003702, 10.09037225, 11.00246762],\n       [ 7.04917473,  8.05262552,  9.0596366 , 10.00519575, 10.08950895,\n         8.07282662,  7.081835  ,  8.05002228],\n       [11.08101894, 10.00959685, 12.021895  , 14.02587191, 14.04681058,\n        10.04593732, 11.07095098, 10.0178053 ],\n       [ 9.05314499, 10.01677422, 11.07688139, 12.09281705, 12.06094937,\n         9.01501835,  8.04896267, 10.0377345 ],\n       [10.08486014, 11.09110972, 13.03838487, 14.03154959, 14.05683942,\n        10.0187818 , 10.01258415, 11.06875958]])\n\n\n\nA\n\narray([[11.09940146, 11.06768737, 12.07908225, 14.01709143, 14.00268493,\n        11.08003702, 10.09037225, 11.00246762],\n       [ 7.04917473,  8.05262552,  9.0596366 , 10.00519575, 10.08950895,\n         8.07282662,  7.081835  ,  8.05002228],\n       [11.08101894, 10.00959685, 12.021895  , 14.02587191, 14.04681058,\n        10.04593732, 11.07095098, 10.0178053 ],\n       [ 9.05314499, 10.01677422, 11.07688139, 12.09281705, 12.06094937,\n         9.01501835,  8.04896267, 10.0377345 ],\n       [10.08486014, 11.09110972, 13.03838487, 14.03154959, 14.05683942,\n        10.0187818 , 10.01258415, 11.06875958]])\n\n\nThey are the same!\n\n\nApproximating \\(A\\) using Smaller Matrices\nOne of the main reasons why SVD is so useful is because we can get a pretty good approximation of the matrix \\(A\\) by choosing a smaller subset of these decomposed matrices: \\(U\\), \\(D\\), and \\(V\\). For instance, in our original situation: \\[\nA = UDV^{T} = \\{m \\times m\\} \\times \\{m \\times n\\} \\times \\{n \\times n\\} = \\{m \\times n\\}\n\\] However, we can choose a subset say \\(k\\) = 3, where we will only:\n\n\nPick the first \\(k\\) columns of U\n\nPick the top \\(k\\) singular values of D\n\nPick the first \\(k\\) rows of V\n\nWhich gives us: \\[\nA = UDV^{T} = \\{m \\times k\\} \\times \\{k \\times k\\} \\times \\{k \\times n\\} = \\{m \\times n\\}\n\\] Therefore, we can get a \\(m \\times n\\) approximation of \\(A\\) using a smaller subset of these decomposed matrices. The way this works is that in SVD, the obtained singular values are present in decreasing order of “importance”. This means that the first singular value is typically the most important in terms of capturing the primary patterns, structures, and variations of the data, and this “importance” keeps on decreasing as we go down. Therefore, depending on our value of \\(k\\), we can get a pretty good approximation of the original matrix using a relatively smaller subset of the decomposed matrices. To see this in practice, let us create a function that will allow us to visualize our: original matrix \\(A\\) and the reconstructed matrix \\(A\\) with a smaller subset of the decomposed matrices, as images. This will allow us to get an appreciation of how similar the reconstructed matrix \\(A\\) is to the original one, while saving us a lot of space because we used a smaller subset of the matrices:\n\ndef compareImages(A, A_):\n    fig, axarr = plt.subplots(1, 2, figsize = (15, 5))\n    \n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"Original Image\")\n    \n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    axarr[1].set(title = \"Reconstructed Image\")\n\n#creating subsets of the decomposed matrices\nk = 1\nU_ = U[:,:k]\nD_ = D[:k, :k]\nV_ = V[:k, :]\nA_ = U_ @ D_ @ V_\n\n# visualizing the original and reconstructed matrix A (images)\ncompareImages(A, A_)\n\n\n\n\nTherefore, we can see that our reconstructed image is pretty much the same as the original image, and we have reduced the size of our problem from:\n\n\n\\(A_\\text{original} = \\{5 \\times 8\\} = 40 \\text{ units}\\), to\n\n\\(A_\\text{reconstructed} = U_{\\text{k}}D_{\\text{k}}V_{\\text{k}}^{T} = \\{5 \\times 1\\} + \\{1 \\times 1\\} + \\{1 \\times 8\\} = 14 \\text{ units}\\)\n\n\n\n\nImage Compression using Matrix Approximation\nNow that we have understood how using SVD, we can get a fairly good approximation of the matrix \\(A\\) using a much smaller subset of the decomposed matrices, we can go ahead and apply to this to image compression. In simple words, what we are trying to do is:\n\n\nRepresent an image as a matrix,\n\nDecompose the matrix using SVD,\n\nUse a smaller subset of these decomposed matrices to reconstruct the original image (matrix)\n\nFor the purpose of this experiment, the chosen image is:\n\n\n\n\nSpongeBob SquarePants\n\n\n\nNow, let us get started:\n\nimport PIL\nimport urllib\n\n#function to read an image and save it as a numpy array \ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\nurl = \"https://i.pinimg.com/originals/1f/f9/68/1ff9682f61e99f217bb67a61f02ecb56.jpg\"\n\nimg = read_image(url)\n\nnp.shape(img)\n\n(1200, 1920, 3)\n\n\nTherefore, we can see that our image is stored as a numpy object of shape: \\(1200 \\times 1920 \\times 3\\), this means that there are \\(1200\\) rows, \\(1920\\) columns, and the \\(3\\) is representative of the RGB channels - since at each pixel there is a numerical value for all red, green, and blue. To simplify our task of image compression using SVD, we want to deal with a 2-Dimensional matrix. To do this, we can convert this image to greyscale in which case it will have dimensions: \\(1200 \\times 1920\\) - \\(1200\\) rows, \\(1920\\) columns, and each value will range from \\(0\\) to \\(255\\). Where, \\(0\\) = Black, and \\(255\\) = White. Converting our image to greyscale:\n\ndef to_greyscale(image):\n    return 1 - np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\nnp.shape(grey_img)\n\n(1200, 1920)\n\n\nAs mentioned, the image is now a \\(1200 \\times 1920\\) matrix. Now, let us visualize the original and greyscale image side-by-side:\n\nfig, axarr = plt.subplots(1, 2, figsize = (15, 5))\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"Original Image\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"Greyscale Image\")\n\n[Text(0.5, 1.0, 'Greyscale Image')]\n\n\n\n\n\n\nReconstructing the Image from its Singular Value Decomposition\nNow, we will write the function svd_reconstruct which will reconstruct an image using \\(k\\) singular values:\n\ndef svd_reconstruct(image, k):\n    U, D, V = getValues(image)\n    U_ = U[:,:k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    grey_img_recon = U_ @ D_ @ V_\n    return grey_img_recon\n\nNow, we can go ahead and visualize how our reconstructed image would look with, let’s say, \\(k = 50\\) singular values:\n\ncompareImages(grey_img, svd_reconstruct(grey_img, 50))\n\n\n\n\n\n\nExperimenting with Different Values of \\(k\\)\nIn this, part of the code, we first want to modify our svd_reconstruct to incorporate the calculation of how much space we are saving for differing values of \\(k\\). The formula for this would be: \\[\n\\text{Percent Storage} = \\frac{\\text{Storage Needed for Reconstructed}}{\\text{Storage Needed for Original}} = \\frac{\\{(m \\times k) + (k \\times k) + (k \\times n)\\} \\times 8 \\text{ bits}}{m \\times n \\times 8 \\text{ bits}}\n\\]\n\\[\n= (\\frac{mk + kk + kn}{mn}) \\times 100\n\\] We know, that the matrix of our original image contains \\(m \\times n\\) values - each of which is 8 bytes, if we perform SVD and choose \\(k\\) singular values, then we have \\(mk + kk + kn\\) values - each of which is 8 bytes! Thus, the above formula represents the way in which we can achieve percent storage.\nNote: Percent Storage is a measure of what percentage of the original size is the reconstructed image using! So, % Storage = \\(30\\)% means that the reconstructed image’s storage size is \\(30\\)% of that of the original!\n\ndef svd_reconstruct(image, k):\n    U, D, V = getValues(image)\n    U_ = U[:,:k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    grey_img_recon = U_ @ D_ @ V_\n    \n    #code for calculating percent storage\n    reconstructedStorage = (image.shape[0] * k) + (k * k) + (k * image.shape[1])\n    originalStorage = image.shape[0] * image.shape[1]\n    percentStorage = np.round((reconstructedStorage/originalStorage) * 100, 2)\n    \n    #returning reconstructed image, and % storage\n    return grey_img_recon, percentStorage\n\nNow, we want to write our svd_experiment function which will help us look at the effects that varying size of \\(k\\) has on: the quality of the image, and the % storage:\n\ndef svd_experiment(image):\n    rows = 5\n    cols = 4\n    fig, axarr = plt.subplots(rows, cols, figsize = (25, 10))\n    k = [13 * i for i in range(1, 21)]\n    index = 0\n    for i in range(rows):\n        for j in range(cols):\n            #using the value of k at a particular index and then incrementing\n            reconstructedImg, perStorage = svd_reconstruct(image, k[index])\n            index+=1\n            #showing the reconstructed image\n            axarr[i][j].imshow(reconstructedImg, cmap = \"Greys\")\n            axarr[i][j].axis(\"off\")\n            axarr[i][j].set(title = f\"k = {k[index-1]} Singular Values, % Storage = {perStorage}%\")\n    fig.tight_layout()\n    \nsvd_experiment(grey_img)\n\n\n\n\nTherefore, we can see that as the number of \\(k\\) decrease, the quality of the picture becomes blurrier, however the % storage falls. For example in our situation for \\(k = 13\\), the reconstructed image’s size is \\(1.77\\)% of the original image. However, the quality of the image is really low.\nTo get an appreciation for SVD, we should realize that for around \\(k = 104\\), the reconstructed image becomes pretty similar to the original, but the reconstructed image only takes 14.55% the storage that the original one took!\n\n\n\nOptional Extras\nIn this section of the blog post, we want to modify our svd_reconstruct function to allow the user to:\n\n\nSpecify a Desired Compression Factor and Select the Number of Components \\(k\\) based on this Selection: \\[\n        \\text{Compression Factor (CF)} = \\frac{\\text{Size of Compressed Bits}}{\\text{Size of Original Bits}}\n        \\] \\[\n        \\text{CF} =\\frac{mk + k^{2} + kn}{mn} = \\frac{k^2 + k(m+n)}{mn}\n        \\] \\[\n        k^2 + k(m+n) = (mn) \\times \\text{ CF}\n        \\] \\[\n        k^2 + k(m+n) - [(mn) \\times \\text{ CF}] = 0\n        \\] Therefore, we can solve this quadratic equation and get the value for \\(k\\), for the user provided compression factor!\n\nSpecify a Desired Threshold epsilon for the Singular Values: selecting all singular values \\(i\\), such that: \\[\n            \\sigma_{i} \\geq \\epsilon_{\\text{threshold}}\n        \\]\n\nNote: The following implementation assumes that the user will either only specify the (1) compression factor, or only the (2) epsilon factor, or only the explicit value of (3) k - Singular Values\n\ndef svd_reconstruct(image, k, cf = 0, epsilon = 0):\n    grey_img_recon = 0\n    \n    # checking for when compression factor is specified\n    if(cf!=0):\n        # specifying the coefficients of the quadratic equation\n        a = 1\n        b = image.shape[0] + image.shape[1]\n        c = (-1) * cf  * image.shape[0] * image.shape[1]\n        # only looking at the positive value - therefore ignoring the negative\n        k = (-b + np.sqrt((b*b)-(4*a*c)))/(2*a)\n        # rounding the value of k - since we can only have whole values - and converting to integer\n        k = np.round(k)\n        k = k.astype(int)\n        # reconstructing our grey image, based on this value of k \n        U, D, V = getValues(image)\n        U_ = U[:,:k]\n        D_ = D[:k, :k]\n        V_ = V[:k, :]\n        grey_img_recon = U_ @ D_ @ V_\n        \n    # checking for when epsilon value is specified\n    elif(epsilon!=0):\n        U, sigma, V = np.linalg.svd(image)\n        # finding k - the number of singular values which are \n        # above epsilon - the specified threshold \n        k = (sigma[sigma>epsilon]).size\n        D = diagonalize(image, sigma)\n        # reconstructing our grey image, based on this value of k \n        U_ = U[:,:k]\n        D_ = D[:k, :k]\n        V_ = V[:k, :]\n        grey_img_recon = U_ @ D_ @ V_\n    \n    # checking for default case when only \n    elif(epsilon==0 and cf==0):\n        U, D, V = getValues(image)\n        U_ = U[:,:k]\n        D_ = D[:k, :k]\n        V_ = V[:k, :]\n        grey_img_recon = U_ @ D_ @ V_\n        \n    #code for calculating percent storage\n    reconstructedStorage = (image.shape[0] * k) + (k * k) + (k * image.shape[1])\n    originalStorage = image.shape[0] * image.shape[1]\n    percentStorage = np.round((reconstructedStorage/originalStorage) * 100, 2)\n    \n    #returning reconstructed image, and % storage\n    return grey_img_recon, percentStorage\n\n\nExperiments with Different Values of Compression Factor\nNow, we can go ahead get a reconstructed image with a compression factor of say \\(0.05\\)! This means that the reconstructed image, will be \\(0.05 * 100 = 5\\)% of the storage of the actual image:\n\nreconstructedImage, perStorage = svd_reconstruct(grey_img, 0, 0.05)\ncompareImages(grey_img, reconstructedImage)\n\n\n\n\nNow, we can go ahead get a reconstructed image with a compression factor of say \\(0.3\\)! This means that the reconstructed image, will be \\(0.3 * 100 = 30\\)% of the storage of the actual image:\n\nreconstructedImage, perStorage = svd_reconstruct(grey_img, 0, 0.3)\ncompareImages(grey_img, reconstructedImage)\n\n\n\n\nTherefore, we can see that as the compression factor goes up, the reconstructed image becomes more like the original image!\n\n\nExperiments with Different Values of Epsilon - \\(\\epsilon\\)\nFinally, we can experiment providing an \\(\\epsilon\\) - epsilon value, which provides a threshold and only the singular values larger than this specified value will be selected. Let us say, we choose \\(\\epsilon = 13000\\) as our threshold. Therefore, only the singular values which are larger than \\(13000\\) will be used in the image reconstruction:\n\nreconstructedImage, perStorage = svd_reconstruct(grey_img, 0, 0, 13000)\ncompareImages(grey_img, reconstructedImage)\n\n\n\n\nLet us say, we choose \\(\\epsilon = 130\\) as our threshold. Therefore, only the singular values which are larger than \\(130\\) will be used in the image reconstruction:\n\nreconstructedImage, perStorage = svd_reconstruct(grey_img, 0, 0, 130)\ncompareImages(grey_img, reconstructedImage)\n\n\n\n\nTherefore, we can see that as our threshold for the singular values goes down, more and more singular values are used in the image reconstruction, and therefore the reconstructed image becomes more like the original!"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "Systems Programming\n\n\nIn Progress\n\n\n\n\nWriting a Custom Memory Allocator (my own implementation of malloc) in C in an attempt to understand System Calls, and how we can use them more efficiently.\n\n\n\n\n\n\nDec 19, 2023\n\n\nNeil Dcruze\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nThrough machine learning techniques, this project aims to analyze positive and negative emotions by examining tweets\n\n\n\n\n\n\nMay 15, 2023\n\n\nNeil Dcruze\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/mental-health-analysis/index.html",
    "href": "projects/mental-health-analysis/index.html",
    "title": "Analyzing Sentiments from Tweets for Mental Health Support",
    "section": "",
    "text": "Introduction\nSentiment Analysis is an evergrowing field with applications in many areas like analyzing customer feedback, social media monitoring, and so on. However, the primary purpose for this project was to create a sentiment analyis model which can discern “positive” tweets from “negative” tweets. Although, the data set used was the twitter data set, this problem is not limited to that. Having a model which can discern “positive” emotions from “negative” emotions (represented through text) can be really helpful in diagnosing and help people who might be suffering from mental health issues.\nFor the purpose of this project, the model was a binary classifier. This is because the data set which contained the tweets only had two annotations: (0) negative, and (4) positive. However, there have been other works where the range of annotations is higher - very (negative/positive), slightly (negative/positive), and neutral. This multi-class classification might lead to a more nuanced model which can better understand the emotion of a text (Ahmad, Aftab, and Ali 2017). Furthermore, there have also been research in which the SVM has been trained on data from multiple sources, and there has been use of uni-grams and bi-grams in the model. This project will aim to borrow some ideas from this paper, particularly, the use of unigram and bigrams in the term-document matrix (Mullen and Collier 2004). Lastly, there has also been research which inspired future work I could do in this project, including emoticons in the analysis of sentiment of tweets (Go, Bhayani, and Huang 2009).\n\n\nValues Statement\nThe main idea behind this project was to come up with a machine learning model which can potentially be used to intervene and assist people in need of mental health support. Although, one major drawback of this model is that it does not understand the contextual meaning of words.\nI am not particularly sure how this project would be implemented - given that implementing a model which analyzes “text” written by someone can be considered a breach of privacy. However, the potential users for this project could be mental health experts, who want to leverage the power of technology to get a better understanding of people (patients) who visit them. This model will benefit in the diagnosis of people who are suffering from mental health issues and allow intervention. Although this project does not directly harm anyone, the fact that it is trained over a data set which contained tweets in English means that anyone who does not know English is actively excluded from the benefits of this model.\nI am really interested in learning about and applying technological solutions which can solve real-life social issues. Due to this, I was really interested in applying my knowledge of machine learning in creating something which could potentially benefit other people. Based on my reflection, if my model had a higher accuracy rate, and if it were implemented respecting an individual’s privacy - my model would make the world a more joyful place!\n\n\nMaterials and Methods\nThe data set chosen for this project is called the Sentiment140 data set, which contains \\(1.6\\) million tweets, which are annotated as Negative (0) and Positive (4). Each row of this data set contains information for a tweet: Sentiment, Tweet ID, Time of Tweet, Username, Tweet, and so on. A potential limitation of this data set is that it only contains tweets which were written in English, therefore, this model excludes people who speak other languages. The chosen model to train this data on was SVM, although, the main question was between: the Primal form of SVM, and the Dual form of SVM. Since we are dealing with linearly separable data, with fewer features compared to samples - the choice was Primal SVM. The Dual form of SVM, would have proven to be very computationally expensive, especially because of the construction of the kernel!\n\nSampling the Data\nBefore, we begin doing anything it is important to understand that the Sentiment140 is a huge data set, which makes many computations very time-consuming. Therefore, we are going to sample the data set to select \\(50000\\) data points (rows) randomly. The code which was used to sample the data is:\nimport pandas as pd\ndf = pd.read_csv(r\"datasetOriginal.csv\", encoding=\"latin-1\")\ndf = df.sample(n=50000)\ndf.to_csv(\"dataset.csv\", index = False)\nSo, now we are working with a dataset which is way more manageable!\n\n\nLoading the Data and Feature Selection\nBefore we perform any of our evaluations, we must prepare our data. For this, we will be using the loadData method from HelperClass:\ndef loadData(self):\n    columns = ['Sentiment Score', 'Tweet ID', 'Time', 'Query', 'Username', 'Tweet']\n    df = pd.read_csv(r\"dataset.csv\", encoding=\"latin-1\", names = columns)\n    df = df[[\"Sentiment Score\", \"Tweet\"]]\n    return df\nIn the above operation, we are assigning adequate feature (column) names to the data. This dataset has a bunch of features, however, we are primarily interested in the:\n\n\nTweet feature: which contains the actual tweet - (Predictor) which will be used for Sentiment Analysis\n\nSentiment Score feature: which contains the true labels - (Target Vector) the actual assigned sentiment for each tweet\n\nNow, we can go ahead and store our data as a pandas data frame:\n\nimport warnings\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nwarnings.filterwarnings('ignore')\nfrom HelperClass import HelperClass\n\nhp = HelperClass()\ndf = hp.loadData()\n\nTo get an understanding of what the data looks like, we can go ahead and examine the first few rows of the data:\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Sentiment Score\n      Tweet\n    \n  \n  \n    \n      0\n      0\n      @switchfoot http://twitpic.com/2y1zl - Awww, t...\n    \n    \n      1\n      0\n      my last day\n    \n    \n      2\n      0\n      my elbow really hurts, whacked it off @lydiaro...\n    \n    \n      3\n      4\n      @MildManneredBoy Well thank you very much sir\n    \n    \n      4\n      4\n      @KylieLuvsJonas we need creeper rings! like yo...\n    \n  \n\n\n\n\nTherefore, we can see that now our data solely consists of the Sentiment Score and the Tweet. To understand the distribution of this data, we can perform:\n\ndf.groupby(\"Sentiment Score\").size()\n\nSentiment Score\n0    24940\n4    25061\ndtype: int64\n\n\nSo, we can clearly see that there are \\(24940\\) tweets which are: Negative (\\(0\\)), and there are \\(25061\\) tweets which are: Positive (\\(4\\))!\n\n\n Preprocessing Tasks \nTo increase accuracy, and to make our input data better, we perform certain “preprocessing” tasks on the raw text data. Peforming the following tasks on the data:\n\n\nCasing: converting everything to either uppercase or lowercase\n\nNoise Removal: eliminating unwanted characters such as HTML tags, punctuation marks, special characters, and so on\n\nTokenization: turning all the tweets into tokens - words separated by spaces\n\nStopword Removal: ignore common English words (“and”, “if”, “for”), that are not likely to shape the sentiment of the tweet\n\nText Normalization (Lemmatization): reducing words to their root form to eliminate variations of the same word\n\n\nBetter \\(\\rightarrow\\) Good\n\nRuns/Running/Ran \\(\\rightarrow\\) Run\n\n\n\nfor i in range(df.shape[0]):\n    df[\"Tweet\"][i] = hp.preprocess(df[\"Tweet\"][i])\n\n\n\nTraining Data, Validation Data and Testing Data\nBefore, we proceed with any operation it is important to divide our data into training data, validation data and testing data:\n\nfrom sklearn.model_selection import train_test_split \ndfX_train, dfX_test, dfY_train, dfY_test = train_test_split(df[\"Tweet\"], df[\"Sentiment Score\"], test_size=0.2, random_state=0)\n\nThe code above splits our entire data: the actual tweets, and the associated sentiment score - into training data and testing data. This data exists in the form of pandas dataframes. The test_size = 0.2, tells the function to save 20% of the data (roughly \\(10,000\\) data points) for testing, and the rest (\\(40,000\\)) for training.\n\ndfX_test, dfX_val, dfY_test, dfY_val = train_test_split(dfX_test, dfY_test, test_size=0.5, random_state=0)\n\nThe code above splits the testing data into 50%, so that the validation data also has around 5000 data points.\nNote: 50,000 Total Data = 40,000 Training (80%) + 5000 Validation (10%) + 5000 Testing (10%) - which is a pretty good split!\n\n\nCreating the Term-Document Matrix and the Target Vector\nNow that we have our training and testing data frames, we want to separate it into the:\n\n\nFeature Matrix (X), and\n\nTarget Vector (y)\n\nLuckily, for us the sentiment score is already numerically encoded (0 = Negative, and 4 = Positive). Although, we will convert this to -1 and 1 (for, 0 and 4 respectively) to standardize how SVM model predictions work. However, we must note that each individual tweet is like an individual row of the Feature Matrix - X. However, currently it is not represented in that format. To achieve that we have to create a “Term-Document Matrix”. A term-document matrix is a matrix in which:\n\n\nColumn: each column represents one term (word) that is present in our complete dataset (corpus) of the tweets\n\nRow: each row contains information about one tweet (document), that is which words are present in that particular tweet\n\nTo think of it simply, all the columns collectively represent all the words that have come up in the complete tweet dataset. Each row, contains tells us, out of all the possible words, which ones are present (and with what frequency) in each individual tweet. Therefore, our term-document matrix will be of the order: \\(\\text{tdm} \\in \\mathbb{R}^{n\\text{x}p}\\), where \\(n\\) = number of tweets, and \\(p\\) = unique words across all tweets.\nImportant Note: in the above description of the columns, all the words in the “complete tweet dataset” refers solely to the training data. This is because when we create the list of vocabulary (all the relevant words present in the tweets), we do not want to look at words in the testing data as that might lead to unwanted biases!\nTo create our term-document matrix, we can use the CountVectorizer from scikit-learn:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\ncv = CountVectorizer(min_df = 0.001, ngram_range=(1,2))\n\nIn the above code, we create an instance of the CountVectorizer class. Here there are some important things to unpack:\n\n\nmin_df: if a term appears in less than 1% of the dataset - ignore it. As we do not have enough data on this term to understand its role in shaping a tweet’s sentiment\n\nngram_range = (1,2): this tells our model to look at uni-grams (one word at a time), and bi-grams (two words taken together). This is good to understand words that often make sense in pairs\n\nNow, we can go ahead and create our term-document matrix (tdm) and appropriate target vector for the training, validation and testing data:\n\nX_train, y_train = hp.prepData(dfX_train, dfY_train, cv, True)\nX_test, y_test = hp.prepData(dfX_test, dfY_test, cv, False)\nX_val, y_val = hp.prepData(dfX_val, dfY_val, cv, False)\n\nThe prep_data function, we use for getting the final feature matrix and target vector is defined as:\ndef prepData(self, dfX, dfY, vectorizer, train = True):\n    if train:\n        vectorizer.fit(dfX)\n    #creating the term document matrix\n    counts = vectorizer.transform(dfX) \n    X = pd.DataFrame(counts.toarray(), columns = vectorizer.get_feature_names_out())\n    X = X.to_numpy()\n    y = dfY.to_numpy()\n    y = np.where(y == 0, -1, np.where(y == 4, 1, y)) \n    return X, y\nThere are some things to unpack here:\n\n\nThe check for training ensures that vocabulary is only created for the training data\n\nBased on this constructed vocabulary, a term-document matrix is constructed (for both training and testing data)\n\nWe convert our feature matrix and target vector to numpy objects for ease\n\nWe change: \\(y = \\{0, 4\\}^{n}\\) to \\(y = \\{-1, 1\\}^{n}\\) for the SVM approach\n\n\n\n\nPEGASOS (Primal Estimated sub-GrAdient SOlver for SVM) : Primal SVM\nIn PEGASOS (Primal Form of SVM), our empirical risk minimization problem is:\n\n\\(\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w})\\;\\)\n\nwhere, our loss function \\(L(\\mathbf{w})\\;\\)is defined as:\n\n\\(L(\\mathbf{w}) = \\frac{\\lambda}{2}||w||^2 + \\frac{1}{n} \\sum_{i = 1}^n \\ell_{\\text{hinge}}(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)\\;\\)\n\ndef loss(self, X, y):\n    y_hat = X@self.w\n    hinge_loss = np.mean(self.hinge_loss(y_hat, y))\n    l2_norm = (self.lamb/2) * np.sum(self.w ** 2)\n    return l2_norm + hinge_loss\nTherefore, we can see that there is an added regularization to prevent overfitting and to increase the model’s ability to generalize over unseen data. For PEGASOS, the loss function which is generally used is called “Hinge Loss”, which is defined as: \\[\n\\ell_{\\text{hinge}}(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i) = \\text{max}\\{0, 1 - y_i\\langle w, x_i \\rangle\\}\n\\]\ndef hinge_loss(self, y_hat, y):\n    return np.maximum(0, 1 - (y * y_hat))\nIn this case, our sub-gradient function is: \\[\n\\nabla L(\\mathbf{w}) = \\lambda w - \\frac{1}{n} \\sum_{i = 1}^n \\mathbb{1}[y_i \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle < 1] y_i x_i\n\\]\ndef gradient_hinge_loss(self, X, y):\n    y_hat = X@self.w \n    tempVal = 1 * ((y_hat * y) < 1)\n\n    output = np.zeros((X.shape[1],))\n    for i in range(X.shape[0]):\n        output += tempVal[i] * np.dot(y[i], X[i])\n    output = output/X.shape[0]\n\n    output = np.dot(self.lamb, self.w) - output\n    return output\nThe following paper provided inspiration for the math behind the PEGASOS model (Shalev-Shwartz et al. 2011).\n\n\nResults\n\nHyperparameter Tuning using Validation Data\nBefore we train the model, we are going to use the validation data to find the ideal hyperparameters (\\(\\lambda\\) - lambda, and batch size) for our model. First let us see, what values of \\(\\lambda\\), yield good result. The \\(\\lambda\\) values would be tested in powers of \\(10\\), while the rest of the hyperparameters remain fixed:\n\nfrom PrimalSVM import PrimalSVM\n\npowers = 10.0 ** np.array([-4, -3, -2, -1, 0, 1])\n\ndf = pd.DataFrame({\"Lambda\": [], \"Training Score\" : [], \"Validation Score\" : []})\n\nfor power in powers:    \n    PS = PrimalSVM(power)\n    PS.fit(X_train, y_train, 0.1, 10000, 500)\n    to_add = pd.DataFrame({\"Lambda\" : [power], \"Training Score\" : [PS.score(X_train, y_train)], \"Validation Score\" : [PS.score(X_val, y_val)]})\n    df = pd.concat((df, to_add))\n\ndf = df.set_index('Lambda')\nprint(df)\n\n         Training Score  Validation Score\nLambda                                   \n0.0001         0.752325          0.729654\n0.0010         0.745025          0.730054\n0.0100         0.717575          0.712458\n0.1000         0.684825          0.679864\n1.0000         0.674825          0.666867\n10.0000        0.634100          0.634473\n\n\nResult 1: Therefore, it seems that \\(\\lambda = 0.001\\) yields the best results on the validation data, so we can expect that this will also yield the best results on the testing data!\nNext, we can see what values of batch size yield good results. We would test for a bunch of different values of batch size, while the other hyperparameters remain fixed:\n\nbatch_size = np.array([25, 50, 100, 250, 500, 650])\n\ndf = pd.DataFrame({\"Batch Size\": [], \"Training Score\" : [], \"Validation Score\" : []})\n\nfor batch in batch_size:\n    PS = PrimalSVM(0.001)\n    PS.fit(X_train, y_train, 0.1, 1000, batch)\n    to_add = pd.DataFrame({\"Batch Size\" : [batch], \"Training Score\" : [PS.score(X_train, y_train)], \"Validation Score\" : [PS.score(X_val, y_val)]})\n    df = pd.concat((df, to_add))\n\ndf = df.set_index('Batch Size')    \nprint(df)\n\n            Training Score  Validation Score\nBatch Size                                  \n25.0              0.745375          0.728854\n50.0              0.745400          0.727455\n100.0             0.745350          0.729654\n250.0             0.745175          0.728654\n500.0             0.745475          0.731054\n650.0             0.745025          0.731254\n\n\nResult 2: Although the results are pretty close, it seems that the score stabilizes around batch size = \\(500\\). So we can expect that this will also yield the best results on the testing data!\n\n\nTraining the Model\nNow, that we have an understanding of how the PEGASOS (Primal SVM) method works, and we have the ideal choice for the hyperparameters:\n\n\n\\(\\lambda = 0.001\\)\n\nbatch size = \\(500\\)\n\nWe can fit our model on the training data with the combination of these ideal parameters, and finally test on our testing data, to see what the final results are!\n\nfrom PrimalSVM import PrimalSVM\n\nPS_final = PrimalSVM(0.001)\nPS_final.fit(X_train, y_train, 0.1, 10000, 500)\n\n\ndf = pd.DataFrame({\"Data\": [], \"Score\" : []})\n\nto_add = pd.DataFrame({\"Data\" : [\"Training Data\"], \"Score\" : [np.round(PS_final.score(X_train, y_train), 2)]})\ndf = pd.concat((df, to_add))\n\nto_add = pd.DataFrame({\"Data\" : [\"Validation Data\"], \"Score\" : [np.round(PS_final.score(X_val, y_val), 2)]})\ndf = pd.concat((df, to_add))\n\nto_add = pd.DataFrame({\"Data\" : [\"Testing Data\"], \"Score\" : [np.round(PS_final.score(X_test, y_test), 2)]})\ndf = pd.concat((df, to_add))\n\ndf = df.set_index('Data')  \nprint(df)\n\n                 Score\nData                  \nTraining Data     0.74\nValidation Data   0.73\nTesting Data      0.72\n\n\n\n\nFinal Training Accuracy\nOur model gets a \\(72\\)% score on the unseen Testing Data! Furthermore, our model gets a score of \\(73\\)% on the Validation Data and a score of \\(74\\)% on the Training Data\n\n\nLimitations: Model Accuracy for Ambiguous Sentences\nFirst, let us look at how the trained model does in predicting the sentiments of some sentences which could potentially cause an issue:\n\ntestSentences = [\"I love apples\", \"I hate oranges\", \"Today has been a bad day\", \"Yesterday was a pretty good day\", \"I am smiling\", \"I am happy\", \"I am not happy\", \"I am sad\", \"Coffee is amazing bad bad\"]\n\ndf = pd.DataFrame({\"Sentence\": [], \"Sentiment\" : []})\n\nfor sent in testSentences:\n    tempMatrix = hp.sentencePredict(sent, cv)\n    result = PS_final.predict(tempMatrix)\n    sentiment = \"Negative\"\n    if(result==1):\n        sentiment = \"Positive\"\n    to_add = pd.DataFrame({\"Sentence\" : [sent], \"Sentiment\" : [sentiment]})\n    df = pd.concat((df, to_add))\n\ndf = df.set_index('Sentence')\ndf\n\n\n\n\n\n  \n    \n      \n      Sentiment\n    \n    \n      Sentence\n      \n    \n  \n  \n    \n      I love apples\n      Positive\n    \n    \n      I hate oranges\n      Negative\n    \n    \n      Today has been a bad day\n      Negative\n    \n    \n      Yesterday was a pretty good day\n      Positive\n    \n    \n      I am smiling\n      Positive\n    \n    \n      I am happy\n      Positive\n    \n    \n      I am not happy\n      Positive\n    \n    \n      I am sad\n      Negative\n    \n    \n      Coffee is amazing bad bad\n      Negative\n    \n  \n\n\n\n\nTherefore, our model seems to be doing pretty well for the first \\(6\\) sentences, but as soon as we introduce “not” our model fails! In the sentence “I am not happy”, our model does not contextually understand that “not” negates the sentence - therefore, probably using the weight of the word “happy” to classify the sentence as positive. Furthermore, in the sentence “Coffee is amazing bad bad”, we can see the bag-of-words approach in play, introducing a couple of “bad”s at the end of the sentence, brings down the score of the sentence, making the model classify it as Negative!\n\n\nVisualizing the Evolution of Training Loss\nIn this section of the project, we can go ahead and visualize how the loss of our PEGASOS model evolves over iterations:\n\nnum_steps = len(PS_final.loss_history)\nplt.plot(np.arange(num_steps) + 1, PS_final.loss_history, label = \"PEGASOS Model (Loss History)\")\n\nplt.loglog()\nplt.title(\"Evolution of PEGASOS' Loss over Iterations\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"PEGASOS' Loss\")\n\nlegend = plt.legend() \n\n\n\n\n\n\nComparison to Other Models\nIn this section, we will implement some other models (pre-written in scikit learn) and see how they fare compared to this project’s implementation of PEGASOS:\n\ndf = pd.DataFrame({\"Model\": [], \"Training Score\" : [], \"Testing Score\" : []})\n\nto_add = pd.DataFrame({\"Model\" : [\"PEGASOS\"], \"Training Score\" : [0.74], \"Testing Score\" : [0.72]})\ndf = pd.concat((df, to_add))\n\n# Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier()\nDT.fit(X_train, y_train)\nto_add = pd.DataFrame({\"Model\" : [\"Decision Tree\"], \"Training Score\" : [DT.score(X_train, y_train)], \"Testing Score\" : [DT.score(X_test, y_test)]})\ndf = pd.concat((df, to_add))\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\nto_add = pd.DataFrame({\"Model\" : [\"Logistic Regression\"], \"Training Score\" : [LR.score(X_train, y_train)], \"Testing Score\" : [LR.score(X_test, y_test)]})\ndf = pd.concat((df, to_add))\n\n# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\nSGD = SGDClassifier()\nSGD.fit(X_train, y_train)\nto_add = pd.DataFrame({\"Model\" : [\"Stochastic Gradient Descent\"], \"Training Score\" : [SGD.score(X_train, y_train)], \"Testing Score\" : [SGD.score(X_test, y_test)]})\ndf = pd.concat((df, to_add))\n\n# sorting based on performance on testing data\ndf = df.sort_values(by=['Testing Score'], ascending = False)\n\ndf = df.set_index(\"Model\")\nprint(df)\n\n                             Training Score  Testing Score\nModel                                                     \nLogistic Regression                0.753750         0.7362\nStochastic Gradient Descent        0.750625         0.7270\nPEGASOS                            0.740000         0.7200\nDecision Tree                      0.961400         0.6596\n\n\nTherefore, we can see that in terms of performance on the unseen testing data, this project’s implementation of PEGASOS is on \\(3\\text{rd}\\) position. However, one must note that the difference in the testing score between our model and the other scikit-learn model’s is not that much, and the PEGASOS model fares pretty well with the other models!\n\n\n\nConcluding Discussion\nOverall, I feel that my project met the goals that I had initially set! Although, I was not able to upload my model on a web-app which could be used by the public - that was an extended goal for this project. In terms of its primary goals, I was able to:\n\n\nIdentify the Right Model for Training - PEGASOS\n\nPrepare the Data for the Model\n\nCode and Implement the Model and Train\n\nObtain a relatively good score on the Testing Data\n\nHowever, the model’s score was not as high as I expected (wanted) it to be! Going into this project, I was aware that PEGASOS’ sentiment analysis does not understand the words contextually - however, I wanted to implement the algorithm on my own, for which PEGASOS was a good choice.\nIf I had more time, I would:\n\n\nImplement the use of emoticons while making decisions about the sentiment of the tweets (Go, Bhayani, and Huang 2009)\n\nUse transformers to train a model which understands words contextually, and is able to discern sarcasm from true intent\n\nIf I had more computation power, I would:\n\n\nTrain my model over larger subset of the dataset, to see if that affects our testing score\n\n\n\nPersonal Reflection\nPersonally, I found the process of working on this project very enriching. This class has always been very hands-on, and working on this project was an extension of that. Through this project, I got acquainted with the workflow which is followed in a machine learning project. This was a good exercise to both get the data ready for the machine learning model (identifying and understanding the data set, and then vectorizing it), and to be able to read literature and scholarly papers and translate the math into code. Furthermore, this project made me appreciate the value of writing optimized code and choosing the right models - which do not cause heavy computational pressure on the device. I felt this when I was trying to work with the Dual form of SVM - which required the creation of a really large kernel matrix.\nAll in all, this project definitely has room for improvement, especially with the existence of models which can understand words contextually and discern sarcam from true emotions. However, I am satisfied with my findings in this project, and I feel that I met my goals just right (did not exceed them, and did not fall too short). One goal which I could not meet was uploading this model on a web-app, however, that was an extended goal and I aim to do that soon in the future.\nThis project definitely improved my ability to research machine learning topics, and to teach myself things. Therefore, this experience will definitely help me in my future courses and career. This is because tech is constantly evolving, and to learn new things, you have to be able to teach yourself.\n\n\n\n\n\nReferences\n\nAhmad, Munir, Shabib Aftab, and Iftikhar Ali. 2017. “Sentiment Analysis of Tweets Using SVM.” International Journal of Computer Applications 177 (November): 975–8887. https://doi.org/10.5120/ijca2017915758.\n\n\nGo, Alec, Richa Bhayani, and Lei Huang. 2009. “Twitter Sentiment Classification Using Distant Supervision.” Processing 150 (January).\n\n\nMullen, Tony, and Nigel Collier. 2004. “Sentiment Analysis Using Support Vector Machines with Diverse Information Sources.” In, 412–18.\n\n\nShalev-Shwartz, Shai, Yoram Singer, Nathan Srebro, and Andrew. 2011. Pegasos: Primal Estimated Sub-Gradient Solver for SVM. https://classes.engr.oregonstate.edu/eecs/fall2017/cs534/extra/Pegasos-2011.pdf."
  }
]