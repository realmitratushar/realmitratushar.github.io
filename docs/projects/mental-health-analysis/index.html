<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Neil Dcruze">
<meta name="dcterms.date" content="2023-05-15">
<meta name="description" content="Through machine learning techniques, this project aims to analyze positive and negative emotions by examining tweets">

<title>Analyzing Sentiments from Tweets for Mental Health Support</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html">Blog</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../project.html">Projects</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../work.html">Work</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cv.pdf">CV</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Analyzing Sentiments from Tweets for Mental Health Support</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Machine Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    Through machine learning techniques, this project aims to analyze positive and negative emotions by examining tweets
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Neil Dcruze </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 15, 2023</p>
    </div>
  </div>
    
  </div>
  

</header>

<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>This final project focused on <i>sentiment analysis</i>, more specifically, classifying a given text (tweet in this case) as carrying a “positive” emotion or a “negative” emotion. For the purpose of this classification, this project wrote and implemented the PEGASOS algorithm to solve the primal formulation of SVMs (Support Vector Machines). The primal formulation of SVM is a convex optimization problem that aims to find the optimal hyperplane that maximally separates the classes in a binary classification task. Furthermore, this project dealt with the vectorization of text-based data, and the parameters which affect how text is stored in a way a machine learning model can understand. Through hyperparameter tuning, this project was able to arrive at a <span class="math inline">\(72\)</span>% accuracy on the <i>testing data</i> in predicting the emotion of a text (positive or negative). The limitation of the model created in this project is that it follows a “bag-of-words” approach - therefore, this model does not understand the “contextual” meaning of words.</p>
<p><a href="https://github.com/dcruzeneil/mental-health-analysis"><b>GitHub Repository: <i>Mental Health Analysis</i></b></a></p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Sentiment Analysis is an evergrowing field with applications in many areas like analyzing customer feedback, social media monitoring, and so on. However, the primary purpose for this project was to create a sentiment analyis model which can discern “positive” tweets from “negative” tweets. Although, the data set used was the twitter data set, this problem is not limited to that. Having a model which can discern “positive” emotions from “negative” emotions (represented through text) can be really helpful in diagnosing and help people who might be suffering from mental health issues.</p>
<p>For the purpose of this project, the model was a binary classifier. This is because the data set which contained the tweets only had two annotations: (0) negative, and (4) positive. However, there have been other works where the range of annotations is higher - very (negative/positive), slightly (negative/positive), and neutral. This multi-class classification might lead to a more nuanced model which can better understand the emotion of a text <span class="citation" data-cites="MulticlassSVM">(<a href="#ref-MulticlassSVM" role="doc-biblioref">Ahmad, Aftab, and Ali 2017</a>)</span>. Furthermore, there have also been research in which the SVM has been trained on data from multiple sources, and there has been use of uni-grams and bi-grams in the model. This project will aim to borrow some ideas from this paper, particularly, the use of unigram and bigrams in the term-document matrix <span class="citation" data-cites="unigram">(<a href="#ref-unigram" role="doc-biblioref">Mullen and Collier 2004</a>)</span>. Lastly, there has also been research which inspired future work I could do in this project, including emoticons in the analysis of sentiment of tweets <span class="citation" data-cites="SentimentDistantSupervision">(<a href="#ref-SentimentDistantSupervision" role="doc-biblioref">Go, Bhayani, and Huang 2009</a>)</span>.</p>
</section>
<section id="values-statement" class="level1">
<h1>Values Statement</h1>
<p>The main idea behind this project was to come up with a machine learning model which can potentially be used to intervene and assist people in need of mental health support. Although, one major drawback of this model is that it does not understand the contextual meaning of words.</p>
<p>I am not particularly sure how this project would be implemented - given that implementing a model which analyzes “text” written by someone can be considered a breach of privacy. However, the potential users for this project could be mental health experts, who want to leverage the power of technology to get a better understanding of people (patients) who visit them. This model will benefit in the diagnosis of people who are suffering from mental health issues and allow intervention. Although this project does not directly harm anyone, the fact that it is trained over a data set which contained tweets in English means that anyone who does not know English is actively excluded from the benefits of this model.</p>
<p>I am really interested in learning about and applying technological solutions which can solve real-life social issues. Due to this, I was really interested in applying my knowledge of machine learning in creating something which could potentially benefit other people. Based on my reflection, if my model had a higher accuracy rate, and if it were implemented respecting an individual’s privacy - my model would make the world a more <i>joyful</i> place!</p>
</section>
<section id="materials-and-methods" class="level1">
<h1>Materials and Methods</h1>
<p>The data set chosen for this project is called the <a href="https://www.kaggle.com/datasets/kazanova/sentiment140?datasetId=2477">Sentiment140</a> data set, which contains <span class="math inline">\(1.6\)</span> million tweets, which are annotated as Negative (0) and Positive (4). Each row of this data set contains information for a tweet: Sentiment, Tweet ID, Time of Tweet, Username, Tweet, and so on. A potential limitation of this data set is that it only contains tweets which were written in English, therefore, this model excludes people who speak other languages. The chosen model to train this data on was SVM, although, the main question was between: the <i>Primal</i> form of SVM, and the <i>Dual</i> form of SVM. Since we are dealing with linearly separable data, with fewer features compared to samples - the choice was <i>Primal SVM</i>. The <i>Dual</i> form of SVM, would have proven to be very computationally expensive, especially because of the construction of the <i>kernel</i>!</p>
<section id="sampling-the-data" class="level4">
<h4 class="anchored" data-anchor-id="sampling-the-data"><font color="green">Sampling the Data</font></h4>
<p>Before, we begin doing anything it is important to understand that the <i>Sentiment140</i> is a huge data set, which makes many computations very time-consuming. Therefore, we are going to sample the data set to select <span class="math inline">\(50000\)</span> data points (rows) randomly. The code which was used to sample the data is:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="vs">r"datasetOriginal.csv"</span>, encoding<span class="op">=</span><span class="st">"latin-1"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.sample(n<span class="op">=</span><span class="dv">50000</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>df.to_csv(<span class="st">"dataset.csv"</span>, index <span class="op">=</span> <span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So, now we are working with a dataset which is way more manageable!</p>
</section>
<section id="loading-the-data-and-feature-selection" class="level4">
<h4 class="anchored" data-anchor-id="loading-the-data-and-feature-selection"><font color="green">Loading the Data and Feature Selection</font></h4>
<p>Before we perform any of our evaluations, we must prepare our data. For this, we will be using the <i>loadData</i> method from HelperClass:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loadData(<span class="va">self</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    columns <span class="op">=</span> [<span class="st">'Sentiment Score'</span>, <span class="st">'Tweet ID'</span>, <span class="st">'Time'</span>, <span class="st">'Query'</span>, <span class="st">'Username'</span>, <span class="st">'Tweet'</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.read_csv(<span class="vs">r"dataset.csv"</span>, encoding<span class="op">=</span><span class="st">"latin-1"</span>, names <span class="op">=</span> columns)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df[[<span class="st">"Sentiment Score"</span>, <span class="st">"Tweet"</span>]]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
In the above operation, we are assigning adequate feature (column) names to the data. This dataset has a bunch of features, however, we are primarily interested in the:
<ol>
<li>
<i><u>Tweet</u></i> feature: which contains the actual tweet - (Predictor) which will be used for Sentiment Analysis
</li><li>
<i><u>Sentiment Score</u></i> feature: which contains the true labels - (Target Vector) the actual assigned sentiment for each tweet
</li></ol>
<p>Now, we can go ahead and store our data as a <i>pandas</i> data frame:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> HelperClass <span class="im">import</span> HelperClass</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>hp <span class="op">=</span> HelperClass()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> hp.loadData()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To get an understanding of what the data looks like, we can go ahead and examine the first few rows of the data:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Sentiment Score</th>
      <th>Tweet</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>my last day</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>my elbow really hurts, whacked it off @lydiaro...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>@MildManneredBoy Well thank you very much sir</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>@KylieLuvsJonas we need creeper rings! like yo...</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Therefore, we can see that now our data solely consists of the Sentiment Score and the Tweet. To understand the distribution of this data, we can perform:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df.groupby(<span class="st">"Sentiment Score"</span>).size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>Sentiment Score
0    24940
4    25061
dtype: int64</code></pre>
</div>
</div>
<p>So, we can clearly see that there are <span class="math inline">\(24940\)</span> tweets which are: Negative (<span class="math inline">\(0\)</span>), and there are <span class="math inline">\(25061\)</span> tweets which are: Positive (<span class="math inline">\(4\)</span>)!</p>
</section>
<section id="preprocessing-tasks" class="level4">
<h4 class="anchored" data-anchor-id="preprocessing-tasks"><font color="green"> Preprocessing Tasks </font></h4>
To increase accuracy, and to make our input data better, we perform certain “preprocessing” tasks on the raw text data. Peforming the following tasks on the data:
<ol>
<li>
Casing: converting everything to either uppercase or lowercase
</li><li>
Noise Removal: eliminating unwanted characters such as HTML tags, punctuation marks, special characters, and so on
</li><li>
Tokenization: turning all the tweets into tokens - words separated by spaces
</li><li>
Stopword Removal: ignore common English words (“and”, “if”, “for”), that are not likely to shape the sentiment of the tweet
</li><li>
Text Normalization (Lemmatization): reducing words to their root form to eliminate variations of the same word
<ul>
<li>
Better <span class="math inline">\(\rightarrow\)</span> Good
</li><li>
Runs/Running/Ran <span class="math inline">\(\rightarrow\)</span> Run
</li></ul>
</li></ol>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(df.shape[<span class="dv">0</span>]):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"Tweet"</span>][i] <span class="op">=</span> hp.preprocess(df[<span class="st">"Tweet"</span>][i])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-data-validation-data-and-testing-data" class="level4">
<h4 class="anchored" data-anchor-id="training-data-validation-data-and-testing-data"><font color="green">Training Data, Validation Data and Testing Data</font></h4>
<p>Before, we proceed with any operation it is important to divide our data into <i>training data</i>, <i>validation data</i> and <i>testing data</i>:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>dfX_train, dfX_test, dfY_train, dfY_test <span class="op">=</span> train_test_split(df[<span class="st">"Tweet"</span>], df[<span class="st">"Sentiment Score"</span>], test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The code above splits our entire data: the actual tweets, and the associated sentiment score - into training data and testing data. This data exists in the form of <i>pandas</i> dataframes. The <i>test_size</i> = 0.2, tells the function to save 20% of the data (roughly <span class="math inline">\(10,000\)</span> data points) for testing, and the rest (<span class="math inline">\(40,000\)</span>) for training.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>dfX_test, dfX_val, dfY_test, dfY_val <span class="op">=</span> train_test_split(dfX_test, dfY_test, test_size<span class="op">=</span><span class="fl">0.5</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The code above splits the testing data into 50%, so that the <i>validation data</i> also has around 5000 data points.</p>
<p><b><u>Note</u>: 50,000 Total Data = 40,000 Training (80%) + 5000 Validation (10%) + 5000 Testing (10%)</b> - which is a pretty good split!</p>
</section>
<section id="creating-the-term-document-matrix-and-the-target-vector" class="level4">
<h4 class="anchored" data-anchor-id="creating-the-term-document-matrix-and-the-target-vector"><font color="green">Creating the Term-Document Matrix and the Target Vector</font></h4>
Now that we have our training and testing data frames, we want to separate it into the:
<ul>
<li>
Feature Matrix (X), and
</li><li>
Target Vector (y)
</li></ul>
Luckily, for us the sentiment score is already numerically encoded (0 = Negative, and 4 = Positive). Although, we will convert this to -1 and 1 (for, 0 and 4 respectively) to standardize how SVM model predictions work. However, we must note that each individual tweet is like an individual row of the Feature Matrix - X. However, currently it is not represented in that format. To achieve that we have to create a “Term-Document Matrix”. A term-document matrix is a matrix in which:
<ol>
<li>
Column: each column represents one term (word) that is present in our complete dataset (corpus) of the tweets
</li><li>
Row: each row contains information about one tweet (document), that is which words are present in that particular tweet
</li></ol>
<p>To think of it simply, all the columns collectively represent all the words that have come up in the complete tweet dataset. Each row, contains tells us, out of all the possible words, which ones are present (and with what frequency) in each individual tweet. Therefore, our term-document matrix will be of the order: <span class="math inline">\(\text{tdm} \in \mathbb{R}^{n\text{x}p}\)</span>, where <span class="math inline">\(n\)</span> = number of tweets, and <span class="math inline">\(p\)</span> = unique words across all tweets.</p>
<p><br><u>Important Note</u>: in the above description of the columns, all the words in the “complete tweet dataset” refers solely to the training data. This is because when we create the list of vocabulary (all the relevant words present in the tweets), we do not want to look at words in the <i>testing data</i> as that might lead to unwanted biases!</p>
<p>To create our term-document matrix, we can use the <i>CountVectorizer</i> from <i>scikit-learn</i>:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> CountVectorizer(min_df <span class="op">=</span> <span class="fl">0.001</span>, ngram_range<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
In the above code, we create an instance of the <i>CountVectorizer</i> class. Here there are some important things to unpack:
<ol>
<li>
min_df: if a term appears in less than 1% of the dataset - ignore it. As we do not have enough data on this term to understand its role in shaping a tweet’s sentiment
</li><li>
ngram_range = (1,2): this tells our model to look at uni-grams (one word at a time), and bi-grams (two words taken together). This is good to understand words that often make sense in pairs
</li></ol>
<p>Now, we can go ahead and create our term-document matrix (<i>tdm</i>) and appropriate <i>target vector</i> for the training, validation and testing data:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>X_train, y_train <span class="op">=</span> hp.prepData(dfX_train, dfY_train, cv, <span class="va">True</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>X_test, y_test <span class="op">=</span> hp.prepData(dfX_test, dfY_test, cv, <span class="va">False</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>X_val, y_val <span class="op">=</span> hp.prepData(dfX_val, dfY_val, cv, <span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <i>prep_data</i> function, we use for getting the final <i>feature matrix</i> and <i>target vector</i> is defined as:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prepData(<span class="va">self</span>, dfX, dfY, vectorizer, train <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> train:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        vectorizer.fit(dfX)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#creating the term document matrix</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> vectorizer.transform(dfX) </span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> pd.DataFrame(counts.toarray(), columns <span class="op">=</span> vectorizer.get_feature_names_out())</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X.to_numpy()</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> dfY.to_numpy()</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.where(y <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, np.where(y <span class="op">==</span> <span class="dv">4</span>, <span class="dv">1</span>, y)) </span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
There are some things to unpack here:
<ol>
<li>
The check for training ensures that vocabulary is only created for the training data
</li><li>
Based on this constructed vocabulary, a term-document matrix is constructed (for both training and testing data)
</li><li>
We convert our <i>feature matrix</i> and <i>target vector</i> to numpy objects for ease
</li><li>
We change: <span class="math inline">\(y = \{0, 4\}^{n}\)</span> to <span class="math inline">\(y = \{-1, 1\}^{n}\)</span> for the SVM approach
</li></ol>
</section>
</section>
<section id="pegasos-primal-estimated-sub-gradient-solver-for-svm-primal-svm" class="level1">
<h1>PEGASOS (<font color="green">P</font>rimal <font color="green">E</font>stimated sub-<font color="green">G</font>r<font color="green">A</font>dient <font color="green">SO</font>lver for <font color="green">S</font>VM) : <font color="green">Primal SVM</font></h1>
In PEGASOS (Primal Form of SVM), our empirical risk minimization problem is:<br><br>
<center>
<span class="math inline">\(\hat{\mathbf{w}} = \mathop{\mathrm{arg\,min}}_{\mathbf{w}} \; L(\mathbf{w})\;\)</span>
</center>
<br>where, our loss function <span class="math inline">\(L(\mathbf{w})\;\)</span>is defined as:<br><br>
<center>
<span class="math inline">\(L(\mathbf{w}) = \frac{\lambda}{2}||w||^2 + \frac{1}{n} \sum_{i = 1}^n \ell_{\text{hinge}}(\langle \mathbf{w}, \mathbf{x}_i \rangle, y_i)\;\)</span>
</center>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss(<span class="va">self</span>, X, y):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> X<span class="op">@</span>self.w</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    hinge_loss <span class="op">=</span> np.mean(<span class="va">self</span>.hinge_loss(y_hat, y))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    l2_norm <span class="op">=</span> (<span class="va">self</span>.lamb<span class="op">/</span><span class="dv">2</span>) <span class="op">*</span> np.<span class="bu">sum</span>(<span class="va">self</span>.w <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> l2_norm <span class="op">+</span> hinge_loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><br>Therefore, we can see that there is an added regularization to prevent overfitting and to increase the model’s ability to generalize over unseen data. For PEGASOS, the loss function which is generally used is called “Hinge Loss”, which is defined as:<br><br> <span class="math display">\[
\ell_{\text{hinge}}(\langle \mathbf{w}, \mathbf{x}_i \rangle, y_i) = \text{max}\{0, 1 - y_i\langle w, x_i \rangle\}
\]</span></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> hinge_loss(<span class="va">self</span>, y_hat, y):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, <span class="dv">1</span> <span class="op">-</span> (y <span class="op">*</span> y_hat))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In this case, our sub-gradient function is:<br><br> <span class="math display">\[
\nabla L(\mathbf{w}) = \lambda w - \frac{1}{n} \sum_{i = 1}^n \mathbb{1}[y_i \langle \mathbf{w}, \mathbf{x}_i \rangle &lt; 1] y_i x_i
\]</span></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_hinge_loss(<span class="va">self</span>, X, y):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> X<span class="op">@</span>self.w </span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    tempVal <span class="op">=</span> <span class="dv">1</span> <span class="op">*</span> ((y_hat <span class="op">*</span> y) <span class="op">&lt;</span> <span class="dv">1</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> np.zeros((X.shape[<span class="dv">1</span>],))</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">0</span>]):</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        output <span class="op">+=</span> tempVal[i] <span class="op">*</span> np.dot(y[i], X[i])</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> output<span class="op">/</span>X.shape[<span class="dv">0</span>]</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> np.dot(<span class="va">self</span>.lamb, <span class="va">self</span>.w) <span class="op">-</span> output</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The following paper provided inspiration for the math behind the PEGASOS model <span class="citation" data-cites="MathFormula">(<a href="#ref-MathFormula" role="doc-biblioref">Shalev-Shwartz et al. 2011</a>)</span>.</p>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<section id="hyperparameter-tuning-using-validation-data" class="level4">
<h4 class="anchored" data-anchor-id="hyperparameter-tuning-using-validation-data"><font color="green">Hyperparameter Tuning using Validation Data</font></h4>
<p>Before we train the model, we are going to use the validation data to find the ideal hyperparameters (<span class="math inline">\(\lambda\)</span> - lambda, and <i>batch size</i>) for our model. First let us see, what values of <span class="math inline">\(\lambda\)</span>, yield good result. The <span class="math inline">\(\lambda\)</span> values would be tested in powers of <span class="math inline">\(10\)</span>, while the rest of the hyperparameters remain fixed:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PrimalSVM <span class="im">import</span> PrimalSVM</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>powers <span class="op">=</span> <span class="fl">10.0</span> <span class="op">**</span> np.array([<span class="op">-</span><span class="dv">4</span>, <span class="op">-</span><span class="dv">3</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"Lambda"</span>: [], <span class="st">"Training Score"</span> : [], <span class="st">"Validation Score"</span> : []})</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> power <span class="kw">in</span> powers:    </span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    PS <span class="op">=</span> PrimalSVM(power)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    PS.fit(X_train, y_train, <span class="fl">0.1</span>, <span class="dv">10000</span>, <span class="dv">500</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    to_add <span class="op">=</span> pd.DataFrame({<span class="st">"Lambda"</span> : [power], <span class="st">"Training Score"</span> : [PS.score(X_train, y_train)], <span class="st">"Validation Score"</span> : [PS.score(X_val, y_val)]})</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.concat((df, to_add))</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.set_index(<span class="st">'Lambda'</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         Training Score  Validation Score
Lambda                                   
0.0001         0.752325          0.729654
0.0010         0.745025          0.730054
0.0100         0.717575          0.712458
0.1000         0.684825          0.679864
1.0000         0.674825          0.666867
10.0000        0.634100          0.634473</code></pre>
</div>
</div>
<p><u>Result 1</u>: Therefore, it seems that <span class="math inline">\(\lambda = 0.001\)</span> yields the best results on the <i>validation data</i>, so we can expect that this will also yield the best results on the <i>testing data</i>!</p>
<p>Next, we can see what values of <i>batch size</i> yield good results. We would test for a bunch of different values of <i>batch size</i>, while the other hyperparameters remain fixed:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> np.array([<span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">250</span>, <span class="dv">500</span>, <span class="dv">650</span>])</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"Batch Size"</span>: [], <span class="st">"Training Score"</span> : [], <span class="st">"Validation Score"</span> : []})</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> batch_size:</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    PS <span class="op">=</span> PrimalSVM(<span class="fl">0.001</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    PS.fit(X_train, y_train, <span class="fl">0.1</span>, <span class="dv">1000</span>, batch)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    to_add <span class="op">=</span> pd.DataFrame({<span class="st">"Batch Size"</span> : [batch], <span class="st">"Training Score"</span> : [PS.score(X_train, y_train)], <span class="st">"Validation Score"</span> : [PS.score(X_val, y_val)]})</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.concat((df, to_add))</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.set_index(<span class="st">'Batch Size'</span>)    </span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Training Score  Validation Score
Batch Size                                  
25.0              0.745375          0.728854
50.0              0.745400          0.727455
100.0             0.745350          0.729654
250.0             0.745175          0.728654
500.0             0.745475          0.731054
650.0             0.745025          0.731254</code></pre>
</div>
</div>
<p><u>Result 2</u>: Although the results are pretty close, it seems that the score stabilizes around <i>batch size</i> = <span class="math inline">\(500\)</span>. So we can expect that this will also yield the best results on the <i>testing data</i>!</p>
</section>
<section id="training-the-model" class="level4">
<h4 class="anchored" data-anchor-id="training-the-model"><font color="green">Training the Model</font></h4>
Now, that we have an understanding of how the PEGASOS (Primal SVM) method works, and we have the ideal choice for the hyperparameters:
<ol>
<li>
<span class="math inline">\(\lambda = 0.001\)</span>
</li><li>
<i>batch size</i> = <span class="math inline">\(500\)</span>
</li></ol>
<p>We can fit our model on the training data with the combination of these ideal parameters, and finally test on our <i>testing data</i>, to see what the final results are!</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PrimalSVM <span class="im">import</span> PrimalSVM</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>PS_final <span class="op">=</span> PrimalSVM(<span class="fl">0.001</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>PS_final.fit(X_train, y_train, <span class="fl">0.1</span>, <span class="dv">10000</span>, <span class="dv">500</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"Data"</span>: [], <span class="st">"Score"</span> : []})</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>to_add <span class="op">=</span> pd.DataFrame({<span class="st">"Data"</span> : [<span class="st">"Training Data"</span>], <span class="st">"Score"</span> : [np.<span class="bu">round</span>(PS_final.score(X_train, y_train), <span class="dv">2</span>)]})</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat((df, to_add))</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>to_add <span class="op">=</span> pd.DataFrame({<span class="st">"Data"</span> : [<span class="st">"Validation Data"</span>], <span class="st">"Score"</span> : [np.<span class="bu">round</span>(PS_final.score(X_val, y_val), <span class="dv">2</span>)]})</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat((df, to_add))</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>to_add <span class="op">=</span> pd.DataFrame({<span class="st">"Data"</span> : [<span class="st">"Testing Data"</span>], <span class="st">"Score"</span> : [np.<span class="bu">round</span>(PS_final.score(X_test, y_test), <span class="dv">2</span>)]})</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat((df, to_add))</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.set_index(<span class="st">'Data'</span>)  </span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                 Score
Data                  
Training Data     0.74
Validation Data   0.73
Testing Data      0.72</code></pre>
</div>
</div>
</section>
<section id="final-training-accuracy" class="level4">
<h4 class="anchored" data-anchor-id="final-training-accuracy"><font color="green">Final Training Accuracy</font></h4>
<p>Our model gets a <span class="math inline">\(72\)</span>% score on the unseen <i>Testing Data</i>! Furthermore, our model gets a score of <span class="math inline">\(73\)</span>% on the <i>Validation Data</i> and a score of <span class="math inline">\(74\)</span>% on the <i>Training Data</i></p>
</section>
<section id="limitations-model-accuracy-for-ambiguous-sentences" class="level4">
<h4 class="anchored" data-anchor-id="limitations-model-accuracy-for-ambiguous-sentences"><font color="green">Limitations: Model Accuracy for Ambiguous Sentences</font></h4>
<p>First, let us look at how the trained model does in predicting the sentiments of some sentences which could potentially cause an issue:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>testSentences <span class="op">=</span> [<span class="st">"I love apples"</span>, <span class="st">"I hate oranges"</span>, <span class="st">"Today has been a bad day"</span>, <span class="st">"Yesterday was a pretty good day"</span>, <span class="st">"I am smiling"</span>, <span class="st">"I am happy"</span>, <span class="st">"I am not happy"</span>, <span class="st">"I am sad"</span>, <span class="st">"Coffee is amazing bad bad"</span>]</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"Sentence"</span>: [], <span class="st">"Sentiment"</span> : []})</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> testSentences:</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    tempMatrix <span class="op">=</span> hp.sentencePredict(sent, cv)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> PS_final.predict(tempMatrix)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    sentiment <span class="op">=</span> <span class="st">"Negative"</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(result<span class="op">==</span><span class="dv">1</span>):</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        sentiment <span class="op">=</span> <span class="st">"Positive"</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    to_add <span class="op">=</span> pd.DataFrame({<span class="st">"Sentence"</span> : [sent], <span class="st">"Sentiment"</span> : [sentiment]})</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.concat((df, to_add))</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.set_index(<span class="st">'Sentence'</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Sentiment</th>
    </tr>
    <tr>
      <th>Sentence</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>I love apples</th>
      <td>Positive</td>
    </tr>
    <tr>
      <th>I hate oranges</th>
      <td>Negative</td>
    </tr>
    <tr>
      <th>Today has been a bad day</th>
      <td>Negative</td>
    </tr>
    <tr>
      <th>Yesterday was a pretty good day</th>
      <td>Positive</td>
    </tr>
    <tr>
      <th>I am smiling</th>
      <td>Positive</td>
    </tr>
    <tr>
      <th>I am happy</th>
      <td>Positive</td>
    </tr>
    <tr>
      <th>I am not happy</th>
      <td>Positive</td>
    </tr>
    <tr>
      <th>I am sad</th>
      <td>Negative</td>
    </tr>
    <tr>
      <th>Coffee is amazing bad bad</th>
      <td>Negative</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Therefore, our model seems to be doing pretty well for the first <span class="math inline">\(6\)</span> sentences, but as soon as we introduce “not” our model fails! In the sentence “I am not happy”, our model does not contextually understand that “not” negates the sentence - therefore, probably using the weight of the word “happy” to classify the sentence as positive. Furthermore, in the sentence “Coffee is amazing bad bad”, we can see the bag-of-words approach in play, introducing a couple of “bad”s at the end of the sentence, brings down the score of the sentence, making the model classify it as <i>Negative</i>!</p>
</section>
<section id="visualizing-the-evolution-of-training-loss" class="level4">
<h4 class="anchored" data-anchor-id="visualizing-the-evolution-of-training-loss"><font color="green">Visualizing the Evolution of Training Loss</font></h4>
<p>In this section of the project, we can go ahead and visualize how the loss of our PEGASOS model evolves over iterations:</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>num_steps <span class="op">=</span> <span class="bu">len</span>(PS_final.loss_history)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>plt.plot(np.arange(num_steps) <span class="op">+</span> <span class="dv">1</span>, PS_final.loss_history, label <span class="op">=</span> <span class="st">"PEGASOS Model (Loss History)"</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>plt.loglog()</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Evolution of PEGASOS' Loss over Iterations"</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iterations"</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"PEGASOS' Loss"</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>legend <span class="op">=</span> plt.legend() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="comparison-to-other-models" class="level4">
<h4 class="anchored" data-anchor-id="comparison-to-other-models"><font color="green">Comparison to Other Models</font></h4>
<p>In this section, we will implement some other models (pre-written in scikit learn) and see how they fare compared to this project’s implementation of PEGASOS:</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"Model"</span>: [], <span class="st">"Training Score"</span> : [], <span class="st">"Testing Score"</span> : []})</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>to_add <span class="op">=</span> pd.DataFrame({<span class="st">"Model"</span> : [<span class="st">"PEGASOS"</span>], <span class="st">"Training Score"</span> : [<span class="fl">0.74</span>], <span class="st">"Testing Score"</span> : [<span class="fl">0.72</span>]})</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat((df, to_add))</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision Tree Classifier</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>DT <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>DT.fit(X_train, y_train)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>to_add <span class="op">=</span> pd.DataFrame({<span class="st">"Model"</span> : [<span class="st">"Decision Tree"</span>], <span class="st">"Training Score"</span> : [DT.score(X_train, y_train)], <span class="st">"Testing Score"</span> : [DT.score(X_test, y_test)]})</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat((df, to_add))</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Logistic Regression</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LogisticRegression()</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>LR.fit(X_train, y_train)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>to_add <span class="op">=</span> pd.DataFrame({<span class="st">"Model"</span> : [<span class="st">"Logistic Regression"</span>], <span class="st">"Training Score"</span> : [LR.score(X_train, y_train)], <span class="st">"Testing Score"</span> : [LR.score(X_test, y_test)]})</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat((df, to_add))</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Stochastic Gradient Descent</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> SGDClassifier</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>SGD <span class="op">=</span> SGDClassifier()</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>SGD.fit(X_train, y_train)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>to_add <span class="op">=</span> pd.DataFrame({<span class="st">"Model"</span> : [<span class="st">"Stochastic Gradient Descent"</span>], <span class="st">"Training Score"</span> : [SGD.score(X_train, y_train)], <span class="st">"Testing Score"</span> : [SGD.score(X_test, y_test)]})</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat((df, to_add))</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a><span class="co"># sorting based on performance on testing data</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.sort_values(by<span class="op">=</span>[<span class="st">'Testing Score'</span>], ascending <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.set_index(<span class="st">"Model"</span>)</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                             Training Score  Testing Score
Model                                                     
Logistic Regression                0.753750         0.7362
Stochastic Gradient Descent        0.750625         0.7270
PEGASOS                            0.740000         0.7200
Decision Tree                      0.961400         0.6596</code></pre>
</div>
</div>
<p>Therefore, we can see that in terms of performance on the unseen <i>testing data</i>, this project’s implementation of PEGASOS is on <span class="math inline">\(3\text{rd}\)</span> position. However, one must note that the difference in the testing score between our model and the other scikit-learn model’s is not that much, and the PEGASOS model fares pretty well with the other models!</p>
</section>
</section>
<section id="concluding-discussion" class="level1">
<h1>Concluding Discussion</h1>
Overall, I feel that my project met the goals that I had initially set! Although, I was not able to upload my model on a web-app which could be used by the public - that was an extended goal for this project. In terms of its primary goals, I was able to:
<ol>
<li>
Identify the Right Model for Training - <b>PEGASOS</b>
</li><li>
Prepare the Data for the Model
</li><li>
Code and Implement the Model and Train
</li><li>
Obtain a relatively good score on the <i>Testing Data</i>
</li></ol>
<p>However, the model’s score was not as high as I expected (wanted) it to be! Going into this project, I was aware that PEGASOS’ sentiment analysis does not understand the words contextually - however, I wanted to implement the algorithm on my own, for which PEGASOS was a good choice.</p>
If I had more time, I would:
<ul>
<li>
Implement the use of emoticons while making decisions about the sentiment of the tweets <span class="citation" data-cites="SentimentDistantSupervision">(<a href="#ref-SentimentDistantSupervision" role="doc-biblioref">Go, Bhayani, and Huang 2009</a>)</span>
</li><li>
Use <i>transformers</i> to train a model which understands words contextually, and is able to discern sarcasm from true intent
</li></ul>
If I had more computation power, I would:
<ul>
<li>
Train my model over larger subset of the dataset, to see if that affects our <i>testing score</i>
</li></ul>
</section>
<section id="personal-reflection" class="level1">
<h1>Personal Reflection</h1>
<p>Personally, I found the process of working on this project very enriching. This class has always been very hands-on, and working on this project was an extension of that. Through this project, I got acquainted with the workflow which is followed in a machine learning project. This was a good exercise to both get the data ready for the machine learning model (identifying and understanding the data set, and then vectorizing it), and to be able to read literature and scholarly papers and translate the math into code. Furthermore, this project made me appreciate the value of writing optimized code and choosing the right models - which do not cause heavy computational pressure on the device. I felt this when I was trying to work with the <i>Dual</i> form of SVM - which required the creation of a really large <i>kernel matrix</i>.</p>
<p>All in all, this project definitely has room for improvement, especially with the existence of models which can understand words contextually and discern sarcam from true emotions. However, I am satisfied with my findings in this project, and I feel that I met my goals just right (did not exceed them, and did not fall too short). One goal which I could not meet was uploading this model on a web-app, however, that was an extended goal and I aim to do that soon in the future.</p>
<p>This project definitely improved my ability to research machine learning topics, and to teach myself things. Therefore, this experience will definitely help me in my future courses and career. This is because tech is constantly evolving, and to learn new things, you have to be able to teach yourself.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-MulticlassSVM" class="csl-entry" role="doc-biblioentry">
Ahmad, Munir, Shabib Aftab, and Iftikhar Ali. 2017. <span>“Sentiment Analysis of Tweets Using SVM.”</span> <em>International Journal of Computer Applications</em> 177 (November): 975–8887. <a href="https://doi.org/10.5120/ijca2017915758">https://doi.org/10.5120/ijca2017915758</a>.
</div>
<div id="ref-SentimentDistantSupervision" class="csl-entry" role="doc-biblioentry">
Go, Alec, Richa Bhayani, and Lei Huang. 2009. <span>“Twitter Sentiment Classification Using Distant Supervision.”</span> <em>Processing</em> 150 (January).
</div>
<div id="ref-unigram" class="csl-entry" role="doc-biblioentry">
Mullen, Tony, and Nigel Collier. 2004. <span>“Sentiment Analysis Using Support Vector Machines with Diverse Information Sources.”</span> In, 412–18.
</div>
<div id="ref-MathFormula" class="csl-entry" role="doc-biblioentry">
Shalev-Shwartz, Shai, Yoram Singer, Nathan Srebro, and Andrew. 2011. <em>Pegasos: Primal Estimated Sub-Gradient Solver for SVM</em>. <a href="https://classes.engr.oregonstate.edu/eecs/fall2017/cs534/extra/Pegasos-2011.pdf">https://classes.engr.oregonstate.edu/eecs/fall2017/cs534/extra/Pegasos-2011.pdf</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>